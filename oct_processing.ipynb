{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we have downloaded a net CDF from the Copernicus Data Store: https://cds.climate.copernicus.eu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Changes:\n",
    "\n",
    "- The complete table should be saved in the directory: `data/generated/index_table_output`\n",
    "    - The file name must include the start and end dates for easier identification.\n",
    "\n",
    "- For the functions `generate_etccdi_temporal_tables` and `generate_etccdi_temporal_tables__centroid`:\n",
    "    - Suppress the printing of each graphic iteration during execution to reduce unnecessary output.\n",
    "\n",
    "- Ensure that all generated graphics reference a single, consistent scale (legend).\n",
    "\n",
    "- Issue with validation if:\n",
    "    - partial years are set in the parameters option \n",
    "    - ex. monthly data is allowed but parameters request only part of given year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do List\n",
    "\n",
    "## dependencies file\n",
    "- This is started as requirements.tx\n",
    "    - Go through each function (.py) and pull out all dependencies \n",
    "    - compile into one simplified list\n",
    "    - consult viewser and check which are absent\n",
    "    - make list of just the absent to install before running\n",
    "\n",
    "## Decision Tree Development\n",
    "- Create a **decision tree for defensible methods** based on different applications:\n",
    "    - If performing at an admin or country scale, use method X.\n",
    "    - Address the question: \"At what scale does the utility of finer-grained PRIOgrid data diminish?\" \n",
    "    - Incorporate considerations of 'other' shapefile extents.\n",
    "\n",
    "## API Considerations\n",
    "- Investigate potential **API changes**:\n",
    "    1. Ensure the `define_request.py` script is correctly referenced.\n",
    "    2. Contact CDS to determine their API update schedule (e.g., every 6 months or yearly).\n",
    "    3. Check the **CDS API forum** for transparency on updates.\n",
    "\n",
    "## Feature Enhancements\n",
    "- Add **descriptive statistics**:\n",
    "    - Include basic summary stats for the requested indices.\n",
    "- Allow users to view country-specific data:\n",
    "    - Avoid packaging into larger datasets.\n",
    "  \n",
    "## Additional Notes\n",
    "- Add a **section in the documentation** clarifying how to migrate workflows toward ingestion smoothly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingester3.extensions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Scaffolders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.4 Scaffolders\n",
    "\n",
    "new_pgy = pd.DataFrame.pgy.new_structure()\n",
    "new_pgm = pd.DataFrame.pgm.new_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pgy = new_pgy.rename(columns={'year_id':'year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pgy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pgm['month'] = new_pgm.m.month\n",
    "new_pgm['year'] = new_pgm.m.year\n",
    "\n",
    "new_pgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils path added to Python Path: /Users/gbenz/Documents/Climate Data/climate_extremes/utils\n"
     ]
    }
   ],
   "source": [
    "from setup_environment import setup_utils_path\n",
    "setup_utils_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format API Request ----------------------------------------------------------------\n",
    "from unzip import unzip_etccdi_package\n",
    "from correct_longitude import transform_longitudinal_values\n",
    "from temporal_index import find_etccdi_timeindex, translate_index_to_daterange\n",
    "from define_request import generate_and_validate_request\n",
    "\n",
    "# Provide Metadata ------------------------------------------------------------------\n",
    "from give_metadata import give_metadata\n",
    "\n",
    "# Build API Request -----------------------------------------------------------------\n",
    "from cds_api_pull import pull_from_cds_api\n",
    "\n",
    "# Methods ---------------------------------------------------------------------------\n",
    "from etccdi_to_pg__pointquery import generate_etccdi_temporal_tables__centroid\n",
    "from etccdi_to_pg import generate_etccdi_temporal_tables\n",
    "\n",
    "# Validation ------------------------------------------------------------------------\n",
    "from give_reference_frame import provide_reference_frame\n",
    "from id_null_values import report_null_etccdi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access with Copernicus Data Store API:\n",
    "\n",
    "### You must have both a Copernicus Data Store account and have followed the proceeding instructions on setting up the CDSAPI before you can retrieve and process data from this toolbox. \n",
    "\n",
    "#### The following instructions reference the CDSAPI set up guide: https://cds.climate.copernicus.eu/how-to-api\n",
    "\n",
    "\n",
    "The final objective is to construct a main.py function that accepts the parameters below and automatically computes the process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request and Processing Parameters\n",
    "\n",
    "### API Request Parameters\n",
    "These parameters define the data retrieved via the API request:\n",
    "- **`variable`**: Specifies the data variable of interest.\n",
    "- **`product_type`**: Indicates the base product type.\n",
    "- **`experiment`**: Defines the experimental setup or scenario.\n",
    "- **`temporal_aggregation`**: Determines the temporal resolution of the data.\n",
    "\n",
    "### Processing Parameters\n",
    "These parameters guide the processing workflow:\n",
    "- **`start_year`**: `'1995'`\n",
    "- **`start_month`**: `'01'`\n",
    "- **`end_year`**: `'2000'`\n",
    "- **`end_month`**: `'12'`\n",
    "\n",
    "### Method Selection\n",
    "Choose a resampling or data alignment method:\n",
    "- **Bilinear sampling**: Applies weighted interpolation for smoother results.\n",
    "- **Point neighbor**: Maps the closest neighboring value preserving the original coarse resolution.\n",
    "\n",
    "### (In Development) Extent Selection\n",
    "Additional options for spatial analysis:\n",
    "- Empirical distributions based on basin-specific extents.\n",
    "- Future iterations will enable a 'hotspot' methodology for comparison with global metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of CDS Request\n",
    "\n",
    "```\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "request = {\n",
    "    \"variable\": [\"cold_days\"], \n",
    "    \"product_type\": [\"base_period_1961_1990\"],\n",
    "    \"model\": [\"hadgem3_gc31_ll\"],\n",
    "    \"ensemble_member\": [\"r1i1p1f3\"],\n",
    "    \"experiment\": [\"ssp1_2_6\"],\n",
    "    \"temporal_aggregation\": [\"monthly\"],\n",
    "    \"period\": [\"201501_210012\"],\n",
    "    \"version\": [\"2_0\"]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download()\n",
    "```   \n",
    "``` \n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "request = {\n",
    "    \"variable\": [\"consecutive_dry_days\"],\n",
    "    \"product_type\": [\"base_independent\"],\n",
    "    \"model\": [\"hadgem3_gc31_ll\"],\n",
    "    \"ensemble_member\": [\"r1i1p1f3\"],\n",
    "    \"experiment\": [\"ssp1_2_6\"],\n",
    "    \"temporal_aggregation\": [\"yearly\"],\n",
    "    \"period\": [\"2015_2100\"],\n",
    "    \"version\": [\"2_0\"]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informing parameters\n",
    "\n",
    "This series of prompts will help constrain appropriate parameters to construct a request that matches the CDS API\n",
    "\n",
    "First, select a temporal aggregation (yearly / monthly)\n",
    "\n",
    "- if yearly:\n",
    "supply a response that reads: (all variables are available at yearly temporal resolution. Here a list of all available climate indices.)\n",
    "\n",
    "- if monthly:\n",
    "supply a response that reads: (Select indices are available at a monthly temporal resolution. Select from the following:)\n",
    "\n",
    "\n",
    "supply the climate index from the list of available variables. Please provide the name exactly as it is written in the list.\n",
    "\n",
    "- add code check (Is the variable located within the list?) \n",
    "    - if not, supply prompt to check spelling\n",
    "\n",
    "Next, select the climate 'experiment' used to process the derived climate indices. You may select from 'historical, ssp2.. sspx... and sspz\n",
    "\n",
    "- if historical AND monthly\n",
    "    - align appropriate 'period' variable\n",
    "    - etcd (applied to sspx_x and so on)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data = {\n",
    "    \"product_type\": [\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_period_1961_1990\",\n",
    "        \"base_period_1961_1990\", \"base_period_1961_1990\", \"base_period_1961_1990\"\n",
    "    ],\n",
    "    \"variable\": [\n",
    "        \"consecutive_dry_days\", \"consecutive_wet_days\", \"diurnal_temperature_range\", \"frost_days\",\n",
    "        \"growing_season_length\", \"heavy_precipitation_days\", \"ice_days\", \"maximum_1_day_precipitation\",\n",
    "        \"maximum_5_day_precipitation\", \"maximum_value_of_daily_maximum_temperature\",\n",
    "        \"minimum_value_of_daily_maximum_temperature\", \"maximum_value_of_daily_minimum_temperature\",\n",
    "        \"minimum_value_of_daily_minimum_temperature\", \"number_of_wet_days\", \"simple_daily_intensity_index\",\n",
    "        \"summer_days\", \"total_wet_day_precipitation\", \"tropical_nights\", \"very_heavy_precipitation_days\",\n",
    "        \"cold_days\", \"cold_nights\", \"warm_days\", \"warm_nights\"\n",
    "    ],\n",
    "    \"temporal_aggregation\": [\n",
    "        \"annual\", \"annual\", \"monthly or annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"monthly or annual\",\n",
    "        \"monthly or annual\", \"monthly or annual\", \"monthly or annual\", \"monthly or annual\", \"monthly or annual\",\n",
    "        \"annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"monthly or annual\", \"monthly or annual\",\n",
    "        \"monthly or annual\", \"monthly or annual\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "#first, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First, select a temporal aggregation (yearly \\ monthly)')\n",
    "\n",
    "time = 'monthly'\n",
    "\n",
    "if time == 'monthly':\n",
    "    variable_list = df.loc[df['temporal_aggregation'].str.contains('monthly or annual'), 'variable'].tolist()\n",
    "\n",
    "    print('Select indices are available at a monthly temporal resolution. Select from the following:')\n",
    "    print()\n",
    "    print(variable_list)\n",
    "\n",
    "if time == 'yearly':\n",
    "    \n",
    "    variable_list = df['variable'].tolist()\n",
    "    print('all variables are available at yearly temporal resolution. Here a list of all available climate indices.')\n",
    "    print()\n",
    "    print(variable_list)\n",
    "\n",
    "variable = 'maximum_1_day_precipitation'\n",
    "\n",
    "#define the product type:\n",
    "\n",
    "product_type = df.loc[df['variable'] == variable, 'product_type'].values[0]\n",
    "\n",
    "if variable in variable_list:\n",
    "    print(f\"'{variable}' is a valid selection.\")\n",
    "else:\n",
    "    raise ValueError(f\"'{variable}' does not in the list. Please check your spelling!\")\n",
    "\n",
    "print('Finally, select the climate experiment used to process the derived climate indices. You may select from historical, ssp2.. sspx... and sspz')\n",
    "\n",
    "user_scenario = 'historical'\n",
    "\n",
    "if user_scenario == \"historical\" and time == \"monthly\":\n",
    "    result = \"185001_201412\"\n",
    "elif user_scenario == \"historical\" and time == \"yearly\":\n",
    "    result = \"1850_2014\"\n",
    "elif user_scenario in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"] and time == \"monthly\":\n",
    "    result = \"201501_210012\"\n",
    "elif user_scenario in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"] and time == \"yearly\":\n",
    "    result = \"2015_2100\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid combination of scenario '{user_scenario}' and time '{time}'.\")\n",
    "\n",
    "\n",
    "# Split the string by the underscore\n",
    "split_string = result.split('_') \n",
    "\n",
    "# Extract the first four characters of each element and convert them to integers\n",
    "min_value = int(split_string[0][:4])  # First four characters, converted to integer\n",
    "max_value = int(split_string[1][:4])  # First four characters, converted to integer\n",
    "\n",
    "# Define the user input (this is an example; replace with actual user input)\n",
    "user_input = \"1900\"  # This should be a string input, like from an input prompt\n",
    "\n",
    "# Convert the user input to an integer\n",
    "input_value = int(user_input)\n",
    "\n",
    "# Check if the input is within the range\n",
    "if min_value <= input_value <= max_value:\n",
    "    print(f\"The input value {input_value} is within the range {min_value}-{max_value}.\")\n",
    "else:\n",
    "    print(f\"The input value {input_value} is out of the permitted range {min_value}-{max_value}.\")\n",
    "\n",
    "print(f'acceptable years for the selected Climate Scenario ({user_scenario}) are {min_value}-{max_value}')\n",
    "#Check if temporal inputs are valid! \n",
    "#    appropriate range is defined by 'result'\n",
    "\n",
    "#is the start year within appropriate range?\n",
    "\n",
    "#is the end year within appropriate range?\n",
    "\n",
    "# Start a loop to repeatedly ask for input until a valid value is entered\n",
    "while True:\n",
    "    # Define the user input (this will be from an input prompt)\n",
    "    user_input = input(f\"Please enter a value between {min_value} and {max_value}: \")\n",
    "    \n",
    "    try:\n",
    "        # Convert the user input to an integer\n",
    "        input_value = int(user_input)\n",
    "\n",
    "        # Check if the input is within the range\n",
    "        if min_value <= input_value <= max_value:\n",
    "            print(f\"The input value {input_value} is within the range {min_value}-{max_value}.\")\n",
    "            break  # Exit the loop if the input is valid\n",
    "        else:\n",
    "            print(f\"The input value {input_value} is out of the permitted range {min_value}-{max_value}. Please try again.\")\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a valid number.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Type and Variable Temporal Aggregation\n",
    "\n",
    "| product_type            | variable                           | temporal_aggregation |\n",
    "|-------------------------|------------------------------------|----------------------|\n",
    "| base_independent         | consecutive_dry_days               | annual               |\n",
    "| base_independent         | consecutive_wet_days               | annual               |\n",
    "| base_independent         | diurnal_temperature_range          | monthly or annual    |\n",
    "| base_independent         | frost_days                         | annual               |\n",
    "| base_independent         | growing_season_length              | annual               |\n",
    "| base_independent         | heavy_precipitation_days           | annual               |\n",
    "| base_independent         | ice_days                           | annual               |\n",
    "| base_independent         | maximum_1_day_precipitation        | monthly or annual    |\n",
    "| base_independent         | maximum_5_day_precipitation        | monthly or annual    |\n",
    "| base_independent         | maximum_value_of_daily_maximum_temperature | monthly or annual    |\n",
    "| base_independent         | minimum_value_of_daily_maximum_temperature | monthly or annual    |\n",
    "| base_independent         | maximum_value_of_daily_minimum_temperature | monthly or annual    |\n",
    "| base_independent         | minimum_value_of_daily_minimum_temperature | monthly or annual    |\n",
    "| base_independent         | number_of_wet_days                 | annual               |\n",
    "| base_independent         | simple_daily_intensity_index       | annual               |\n",
    "| base_independent         | summer_days                        | annual               |\n",
    "| base_independent         | total_wet_day_precipitation        | annual               |\n",
    "| base_independent         | tropical_nights                    | annual               |\n",
    "| base_independent         | very_heavy_precipitation_days      | annual               |\n",
    "| base_period_1961_1990    | cold_days                          | monthly or annual    |\n",
    "| base_period_1961_1990    | cold_nights                        | monthly or annual    |\n",
    "| base_period_1961_1990    | warm_days                          | monthly or annual    |\n",
    "| base_period_1961_1990    | warm_nights                        | monthly or annual    |\n",
    "\n",
    "The table above defines the `product_type`, `variable`, and the corresponding `temporal_aggregation` classification options for required parameters. Variables that are only available on a yearly basis are labeled as \"annual,\" while others that can be reported on both monthly and annual bases are labeled as \"monthly or annual.\"\n",
    "\n",
    "### Experiment Period Mapping\n",
    "\n",
    "| experiment  | period        |\n",
    "|-------------|---------------|\n",
    "| historical | 185001_201412 |\n",
    "| ssp1_2_6   | 201501_210012 |\n",
    "| ssp2_4_5   | 201501_210012 |\n",
    "| ssp5_8_5   | 201501_210012 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from the .txt file\n",
    "import os\n",
    "base_dir= os.getcwd()\n",
    "config_file_path = f'{base_dir}/request.txt'  # Adjust this path to where your .txt file is located\n",
    "\n",
    "config = {}\n",
    "with open(config_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        config[key.strip()] = value.strip()\n",
    "\n",
    "# Assign variables from the config dictionary\n",
    "p_variable = config.get('p_variable')\n",
    "p_product_type = config.get('p_product_type')\n",
    "p_experiment = config.get('p_experiment')\n",
    "p_temporal_aggregation = config.get('p_temporal_aggregation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_variable = \"ice_days\"\n",
    "p_product_type=\"base_independent\"\n",
    "p_experiment=\"ssp1_2_6\"\n",
    "p_temporal_aggregation=\"yearly\"\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Define Start Year & Month\n",
    "#-----------------------------------------------------------\n",
    "start_year = '2015'\n",
    "start_month = '01'\n",
    "#-----------------------------------------------------------\n",
    "# Define End Year & Month\n",
    "end_year = '2016'\n",
    "end_month = '04'\n",
    "#-----------------------------------------------------------\n",
    "method = 'raster_query' # or raster_query\n",
    "#-----------------------------------------------------------\n",
    "save_tif = 'no' # or no\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "#country_selection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request is valid.\n",
      "yearly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'variable': ['ice_days'],\n",
       " 'product_type': ['base_independent'],\n",
       " 'model': ['hadgem3_gc31_ll'],\n",
       " 'ensemble_member': ['r1i1p1f3'],\n",
       " 'experiment': ['ssp1_2_6'],\n",
       " 'temporal_aggregation': ['yearly'],\n",
       " 'period': ['2015_2100'],\n",
       " 'version': ['2_0'],\n",
       " 'data_format': 'netcdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, calling the function will generate and validate the request\n",
    "\n",
    "request = generate_and_validate_request(\n",
    "    variable=p_variable,\n",
    "    product_type=p_product_type,\n",
    "    experiment=p_experiment,\n",
    "    temporal_aggregation=p_temporal_aggregation\n",
    ")\n",
    "temporal_aggregation_value = request['temporal_aggregation'][0]\n",
    "print(temporal_aggregation_value)\n",
    "display(request)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Load a clean PG dataframe at a consistent temporal resolution\n",
    "# to the request built\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#reference_df = provide_reference_frame(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip_file_name will appear in folder:\n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── raw_external/\n",
    "│   │   └── cds_zip/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 14:21:15,929 WARNING [2024-11-19T00:00:00] System is experiencing performance issues. Please check updated status [here](https://status.ecmwf.int/)\n",
      "2024-11-26 14:21:15,930 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-26 14:21:15,930 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-26 14:21:15,931 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-26 14:21:15,931 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-26 14:21:16,686 INFO Request ID is 80be9468-3cb8-4e4d-83b1-4d32a4d5bd00\n",
      "2024-11-26 14:21:16,789 INFO status has been updated to accepted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbenz/Documents/Climate Data/climate_extremes/oct_processing.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m zip_file_name \u001b[39m=\u001b[39m pull_from_cds_api(request)\n",
      "File \u001b[0;32m~/Documents/Climate Data/climate_extremes/utils/cds_api_pull.py:27\u001b[0m, in \u001b[0;36mpull_from_cds_api\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m# Initialize the client and retrieve the file\u001b[39;00m\n\u001b[1;32m     26\u001b[0m client \u001b[39m=\u001b[39m cdsapi\u001b[39m.\u001b[39mClient()\n\u001b[0;32m---> 27\u001b[0m client\u001b[39m.\u001b[39;49mretrieve(dataset, request, target\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(zip_file_path)) \n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m(zip_file_name)\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/cads_api_client/legacy_api_client.py:155\u001b[0m, in \u001b[0;36mLegacyApiClient.retrieve\u001b[0;34m(self, name, request, target)\u001b[0m\n\u001b[1;32m    153\u001b[0m submitted: Remote \u001b[39m|\u001b[39m Results\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait_until_complete:\n\u001b[0;32m--> 155\u001b[0m     submitted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49msubmit_and_wait_on_results(\n\u001b[1;32m    156\u001b[0m         collection_id\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    157\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest,\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     submitted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39msubmit(\n\u001b[1;32m    161\u001b[0m         collection_id\u001b[39m=\u001b[39mname,\n\u001b[1;32m    162\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest,\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/cads_api_client/api_client.py:457\u001b[0m, in \u001b[0;36mApiClient.submit_and_wait_on_results\u001b[0;34m(self, collection_id, **request)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msubmit_and_wait_on_results\u001b[39m(\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m, collection_id: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest: Any\n\u001b[1;32m    443\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m cads_api_client\u001b[39m.\u001b[39mResults:\n\u001b[1;32m    444\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Submit a request and wait for the results to be ready.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39m    cads_api_client.Results\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve_api\u001b[39m.\u001b[39;49msubmit(collection_id, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest)\u001b[39m.\u001b[39;49mmake_results()\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/cads_api_client/processing.py:538\u001b[0m, in \u001b[0;36mRemote.make_results\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_results\u001b[39m(\u001b[39mself\u001b[39m, wait: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Results:\n\u001b[1;32m    537\u001b[0m     \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 538\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_on_results()\n\u001b[1;32m    539\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_api_response(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    540\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/cads_api_client/processing.py:513\u001b[0m, in \u001b[0;36mRemote._wait_on_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults_ready:\n\u001b[1;32m    512\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults not ready, waiting for \u001b[39m\u001b[39m{\u001b[39;00msleep\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 513\u001b[0m     time\u001b[39m.\u001b[39;49msleep(sleep)\n\u001b[1;32m    514\u001b[0m     sleep \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(sleep \u001b[39m*\u001b[39m \u001b[39m1.5\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msleep_max)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zip_file_name = pull_from_cds_api(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The netcdf file will appear in folder: \n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── netcd/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf_file, etccdi_index = unzip_etccdi_package(zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted netcdf file replaced the original file in location:\n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── netcd/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etccdi = transform_longitudinal_values(etccdi_index, netcdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give_metadata(etccdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list, reference_filtered_time, report_temporal_dimensions = translate_index_to_daterange(etccdi, reference_df, temporal_aggregation_value, start_year, start_month, end_year, end_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:\n",
    "\n",
    "- time_index_list,\n",
    "- netcdf, climate_index, \n",
    "- shapefile_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Raster files:\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── index_raster_output/\n",
    "```\n",
    "\n",
    "Final output table:\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── index_table_output/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes:\n",
    "\n",
    "#### Do not print each iteration of the graphics!\n",
    "- Complete for raster_query\n",
    "- Finish for resample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.enums import Resampling\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import math  # Add this import at the top of your file\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages  # Import PdfPages for saving PDF layouts\n",
    "import tempfile\n",
    "\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "def generate_layout_and_save(param_time_index_list, plot_figures, output_folder, param_climate_index):\n",
    "    columns = 4\n",
    "    rows = 3\n",
    "    plots_per_page = columns * rows\n",
    "    total_plots = len(plot_figures)\n",
    "    total_pages = math.ceil(total_plots / plots_per_page)\n",
    "\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_folder / f'{param_climate_index}_layout__resample.pdf'\n",
    "\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            fig, axes = plt.subplots(rows, columns, figsize=(11.69, 8.27))  # A4 size in landscape\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i in range(plots_per_page):\n",
    "                plot_index = page * plots_per_page + i\n",
    "                if plot_index < total_plots:\n",
    "                    fig_plot = plot_figures[plot_index]\n",
    "                    \n",
    "                    # Remove x and y labels, but keep the axes and legend\n",
    "                    for ax in fig_plot.get_axes():\n",
    "                        ax.set_xlabel('')  # Remove x-axis label\n",
    "                        ax.set_ylabel('')  # Remove y-axis label\n",
    "\n",
    "                    # Save each figure to a temporary file, then load it\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmpfile:\n",
    "                        fig_plot.savefig(tmpfile.name, bbox_inches='tight')  # Save plot with legend\n",
    "                        img = plt.imread(tmpfile.name)\n",
    "                        axes[i].imshow(img)  # Place the image into the subplot axis\n",
    "                        axes[i].axis('off')  # Turn off axis for a cleaner layout\n",
    "                else:\n",
    "                    axes[i].axis('off')  # Hide unused subplots\n",
    "\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"All graphics saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def generate_etccdi_temporal_tables(param_time_index_list, param_netcdf, param_climate_index, temporal_params, save_raster, param_shapefile_name='pgm_viewser_extent.shp'):\n",
    "    project_root = Path.cwd()  # Set this to your project root manually if needed\n",
    "\n",
    "    map_folder = project_root / 'docs' / 'Graphics' / 'Standard_review'\n",
    "\n",
    "    extent_path = project_root / 'data' / 'processed' / 'extent_shapefile'\n",
    "    extent_filename = extent_path / param_shapefile_name\n",
    "\n",
    "\n",
    "    out_originalraster_folder = project_root / 'data' / 'generated' / 'index_raster_output' /'native' \n",
    "    out_upsampleraster_folder = project_root / 'data' / 'generated' / 'index_raster_output' / 'method' / 'upsampled'\n",
    "\n",
    "    generated_index_table_folder = project_root / 'data' / 'generated' / 'index_table_output'\n",
    "\n",
    "    temporal_attribution = '_'.join(temporal_params)\n",
    "\n",
    "    all_stats = []\n",
    "    plot_figures = []  # Initialize list to store figures\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    #os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Retrieve the first and last time indices for file naming\n",
    "    first_time_index = param_time_index_list[0]\n",
    "    last_time_index = param_time_index_list[-1]\n",
    "\n",
    "    for i in param_time_index_list:\n",
    "        print(f\"Processing time index: {i}\")\n",
    "        \n",
    "        # Select the data for the specified climate index\n",
    "        data = param_netcdf[param_climate_index]\n",
    "        \n",
    "        # Check the data type and process accordingly\n",
    "        data_type = data.dtype\n",
    "        if data_type == 'timedelta64[ns]':\n",
    "            data_days = data / np.timedelta64(1, 'D')  # Convert to days if it's timedelta\n",
    "            raster_data = data_days.isel(time=i)\n",
    "        elif data_type == 'float32':\n",
    "            raster_data = data.isel(time=i)  # Use as-is if it's already float32\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type '{data_type}' for variable '{param_climate_index}'. Expected 'timedelta64[ns]' or 'float32'.\")\n",
    "        \n",
    "        # Convert spatial dimensions\n",
    "        raster_data = raster_data.rename({'lon': 'x', 'lat': 'y'})\n",
    "        raster_data = raster_data.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "\n",
    "        # Get the date and time information\n",
    "        date_time = str(param_netcdf['time'].isel(time=i).values.item())\n",
    "        year, month = date_time.split('-')[:2]\n",
    "        print(\"Year:\", year, \"Month:\", month)\n",
    "\n",
    "        # Set CRS if not already defined\n",
    "        if not raster_data.rio.crs:\n",
    "            print(\"CRS is not set. Setting CRS to EPSG:4326\")\n",
    "            raster_data = raster_data.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Save the original raster to the designated folder\n",
    "        # --- original_raster_path = os.path.join(out_originalraster_folder, f\"original_{param_climate_index}_{year}_{month}.tif\")\n",
    "        # ---- raster_data.rio.to_raster(original_raster_path)\n",
    "        # ---- print(f\"Original raster saved at: {original_raster_path}\")\n",
    "\n",
    "        # Create a separate raster with null values set to -9999\n",
    "        # --- raster_with_nulls_set = raster_data.fillna(-9999)\n",
    "\n",
    "        # Save the modified raster (with nulls as -9999) to the designated folder\n",
    "        # --- null_set_raster_path = os.path.join(out_originalraster_folder, f\"null_set_{param_climate_index}_{year}_{month}.tif\")\n",
    "        # --- raster_with_nulls_set.rio.to_raster(null_set_raster_path)\n",
    "        # --- print(f\"Raster with nulls set to -9999 saved at: {null_set_raster_path}\")\n",
    "\n",
    "        # Resample directly with bilinear interpolation\n",
    "        def resample_with_bilinear(raster_data, factor=3):\n",
    "            # Resample with the target shape\n",
    "            upsampled_raster = raster_data.rio.reproject(\n",
    "                raster_data.rio.crs,\n",
    "                shape=(\n",
    "                    int(raster_data.sizes['y'] * factor),\n",
    "                    int(raster_data.sizes['x'] * factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            return upsampled_raster\n",
    "\n",
    "        # Apply the resampling method\n",
    "        upsampled_raster = resample_with_bilinear(raster_data, factor=20)\n",
    "\n",
    "        # Plot the resampled raster\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        upsampled_raster.plot(cmap='viridis')\n",
    "        plt.title(f'Upsampled Raster for {param_climate_index} at Time Index {i}')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        #plt.show()\n",
    "\n",
    "        if save_raster == 'yes':\n",
    "            upsampled_raster_path = os.path.join(out_upsampleraster_folder, f\"upsampled_{param_climate_index}_{year}_{month}.tif\")\n",
    "            upsampled_raster.rio.to_raster(upsampled_raster_path)\n",
    "            print(f\"Upsampled raster saved at: {upsampled_raster_path}\")\n",
    "        else:\n",
    "        # Save the resampled raster to the designated folder\n",
    "            with MemoryFile() as memfile:\n",
    "                with memfile.open(driver='GTiff', \n",
    "                                width=upsampled_raster.rio.width, \n",
    "                                height=upsampled_raster.rio.height, \n",
    "                                count=1, \n",
    "                                dtype=upsampled_raster.dtype, \n",
    "                                crs=upsampled_raster.rio.crs, \n",
    "                                transform=upsampled_raster.rio.transform()) as dataset:\n",
    "                    dataset.write(upsampled_raster.values, 1)\n",
    "\n",
    "                # Load the shapefile for zonal statistics\n",
    "                gdf = gpd.read_file(extent_filename)\n",
    "                gdf = gdf[['gid', 'geometry', 'xcoord', 'ycoord']]\n",
    "\n",
    "                # Calculate zonal statistics on the upsampled raster\n",
    "                stats = zonal_stats(gdf, memfile, stats='mean', geojson_out=True)\n",
    "                stats_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "                # Add year and month fields\n",
    "                stats_gdf['year'] = year\n",
    "                stats_gdf['month'] = month\n",
    "                stats_gdf.rename(columns={'mean': param_climate_index}, inplace=True)\n",
    "\n",
    "                # Ensure stats_gdf has valid geometry and data\n",
    "                stats_gdf = stats_gdf[stats_gdf.geometry.notnull() & stats_gdf[param_climate_index].notnull()]\n",
    "            del upsampled_raster  # Clean up if no longer needed\n",
    "\n",
    "        # Plot the zonal statistics if there is data\n",
    "        if not stats_gdf.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            stats_gdf.plot(column=param_climate_index, ax=ax, legend=True, cmap='viridis', edgecolor='none')\n",
    "            ax.set_title(f'{param_climate_index} Statistics by Region - {year}-{month}')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "            #plt.show()\n",
    "            plot_figures.append(fig)  # Append figure to list\n",
    "            plt.close(fig)\n",
    "\n",
    "        else:\n",
    "            print(f\"No valid zonal statistics data to plot for {param_climate_index} at time index {i}\")\n",
    "\n",
    "        # Append to list\n",
    "        all_stats.append(stats_gdf)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    final_gdf = pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "    # Save final DataFrame to CSV in the designated folder\n",
    "\n",
    "    file_name = f\"{param_climate_index}_{temporal_attribution}__centroid_process.csv\"\n",
    "    output_file_path = generated_index_table_folder / file_name\n",
    "\n",
    "\n",
    "    final_gdf.to_csv(output_file_path, index=False)\n",
    "    print(f\"Final DataFrame saved to: {output_file_path}\")\n",
    "\n",
    "    generate_layout_and_save(param_time_index_list, plot_figures, map_folder, param_climate_index)\n",
    "\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'raster_query':\n",
    "    translated_filename = generate_etccdi_temporal_tables__centroid(index_list, etccdi, etccdi_index, report_temporal_dimensions, save_tif)\n",
    "\n",
    "elif method == 'resample':\n",
    "    translated_filename = generate_etccdi_temporal_tables(index_list, etccdi, etccdi_index, report_temporal_dimensions, save_tif)\n",
    "\n",
    "else: \n",
    "    print('you have entered a bad prompt for the method parameter. Please restart.... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``translated_filename`` retrieves the summary table saved to:\n",
    "\n",
    "\n",
    "``reference_filtered_time`` retrieves the primary reference table saved to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There was an error in report_null_etccdi_values:\n",
    "proceeding code line concentrates on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = str(translated_filename).split('_')[0]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_null_etccdi_values(translated_filename, reference_filtered_time, temporal_aggregation_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you can migrate to the ingestion script\n",
    "\n",
    "We want to intentionally keep these things seperate (lock / key) so unwanted things are not automatically ingested\n",
    "\n",
    "- **Clarify migration to the ingestion script**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Now you are ready to run the ingestion code located in .... \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
