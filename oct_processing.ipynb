{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we have downloaded a net CDF from the Copernicus Data Store: https://cds.climate.copernicus.eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = '/Users/gbenz/Downloads/tx10pETCCDI_mon_HadGEM3-GC31-LL_historical_r1i1p1f3_b1981-2010_v20190624_185001-201412_v2-0.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from rasterstats import zonal_stats\n",
    "import numpy as np\n",
    "\n",
    "from utils.unzip import unzip_etccdi_package\n",
    "from utils.correct_longitude import transform_longitudinal_values\n",
    "from utils.give_metadata import give_metadata\n",
    "from utils.temporal_index import find_etccdi_timeindex, translate_index_to_daterange\n",
    "from utils.define_request import generate_and_validate_request\n",
    "\n",
    "# Build API Request -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Methods ---------------------------------------------------------------------------\n",
    "from utils.etccdi_to_pg__pointquery import generate_etccdi_temporal_tables__centroid\n",
    "from utils.etccdi_to_pg import generate_etccdi_temporal_tables\n",
    "\n",
    "# Validation ------------------------------------------------------------------------\n",
    "from utils.give_reference_frame import provide_reference_frame\n",
    "from utils.id_null_values import report_null_etccdi_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access with Copernicus Data Store API:\n",
    "\n",
    "### Link for instructions on how to set up the API \n",
    "\n",
    "#### This is a necessary precondition to run the toolbox built here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters:\n",
    "\n",
    "\n",
    "#### Parameters informing the API request:\n",
    "- variable \n",
    "- product_type\n",
    "- experiment\n",
    "- temporal_aggregation\n",
    "\n",
    "#### Parameters informing processing:\n",
    "- start_year = '1995'\n",
    "- start_month = '01'\n",
    "- end_year = '2000'\n",
    "- end_month = '12'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_variable=\"consecutive_dry_days\"\n",
    "p_product_type=\"base_independent\"\n",
    "p_experiment=\"historical\"\n",
    "p_temporal_aggregation=\"yearly\"\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Define Start Year & Month\n",
    "#-----------------------------------------------------------\n",
    "start_year = '1990'\n",
    "start_month = '01'\n",
    "#-----------------------------------------------------------\n",
    "# Define End Year & Month\n",
    "end_year = '1993'\n",
    "end_month = '12'\n",
    "#-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request is valid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'variable': ['consecutive_dry_days'],\n",
       " 'product_type': ['base_independent'],\n",
       " 'model': ['hadgem3_gc31_ll'],\n",
       " 'ensemble_member': ['r1i1p1f3'],\n",
       " 'experiment': ['historical'],\n",
       " 'temporal_aggregation': ['yearly'],\n",
       " 'period': ['1850_2014'],\n",
       " 'version': ['2_0'],\n",
       " 'data_format': 'netcdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priogrid_gid     int64\n",
      "year            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now, calling the function will generate and validate the request\n",
    "request = generate_and_validate_request(\n",
    "    variable=p_variable,\n",
    "    product_type=p_product_type,\n",
    "    experiment=p_experiment,\n",
    "    temporal_aggregation=p_temporal_aggregation\n",
    ")\n",
    "\n",
    "display(request)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Load a clean PG dataframe at a consistent temporal resolution\n",
    "# to the request built\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "reference_df = provide_reference_frame(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 13:52:08,609 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-13 13:52:08,610 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-13 13:52:08,610 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-13 13:52:08,610 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-13 13:52:17,583 INFO Request ID is 0a5ce17c-ef4b-415e-a8d1-fb8b6afd82e4\n",
      "2024-11-13 13:52:17,811 INFO status has been updated to accepted\n",
      "2024-11-13 13:52:20,517 INFO status has been updated to running\n",
      "2024-11-13 13:52:31,551 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e88efedbf34ecf9999c39275573eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "432716f0a4b4f4f0049e5f147da23357.zip:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'consecutive_dry_days_yearly_1850_2014.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cdsapi\n",
    "\n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "\n",
    "# Extract the desired elements from the request dictionary\n",
    "variable = request[\"variable\"][0]\n",
    "temporal_aggregation = request[\"temporal_aggregation\"][0]\n",
    "period = request[\"period\"][0]\n",
    "\n",
    "# Concatenate them with an underscore or any other separator you prefer\n",
    "zip_file_name = f\"{variable}_{temporal_aggregation}_{period}.zip\"\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request, target=zip_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cddETCCDI\n",
      "Extracted file names: cddETCCDI_yr_HadGEM3-GC31-LL_historical_r1i1p1f3_no-base_v20190624_1850-2014_v2-0.nc\n"
     ]
    }
   ],
   "source": [
    "netcdf_file, etccdi_index = unzip_etccdi_package(zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Metadata from the selected ETTCDI netCDF file:\n",
    "\n",
    "Move the pg shapefile to the github repo so this can be accessed without references to local paths\n",
    "\n",
    "Accomplishes:\n",
    "- checks to ensure the correct netcdf file is being processed\n",
    "- provides spatial and temporal metadata\n",
    "\n",
    "From preprocessing, we know that the ETCCDI climate data is not packaged in a desirable format, that is, the original longitudinal range is: 0.9375 to 359.0625\n",
    "- Adjust the Longitude range \n",
    "- save an 'adjusted netcdf' file.\n",
    "\n",
    "\n",
    "28-10 -- What would perhaps be most desirable is to first transform, then, report metadata with two seperate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable 'cddETCCDI' was found in the file path and the world continues to spin.\n",
      "Original Latitude range: -89.375 to 89.375\n",
      "Original Longitude range: 0.9375 to 359.0625\n",
      "Adjusted Longitude range: -179.0625 to 179.0625\n",
      "Adjusted dataset saved to: /Users/gbenz/Downloads/adjusted_cddETCCDI_yr_HadGEM3-GC31-LL_historical_r1i1p1f3_no-base_v20190624_1850-2014_v2-0.nc.nc\n"
     ]
    }
   ],
   "source": [
    "etccdi = transform_longitudinal_values(etccdi_index, netcdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give_metadata(etccdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list, reference_filtered_time = translate_index_to_daterange(etccdi, reference_df, start_year, start_month, end_year, end_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the first n elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_index = index_list[:2]\n",
    "\n",
    "time_length_subset = len(sub_index)\n",
    "time_length = len(index_list)\n",
    "\n",
    "print(time_length_subset)\n",
    "print(time_length)\n",
    "\n",
    "print(sub_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puts it all together\n",
    "\n",
    "Parameters:\n",
    "1. references the sub_index which supplies the list (or sublist) of indexes to iterate over. Index specifically references time. This can be confusing because the ETCCDI variables are themselves climate indices.\n",
    "2. Creates a single geotiff from the current time selection. We do this because the NetCDF itself is not a format that can be incorporated into rigorous analysis so as we iterate through the time series we convert the working item to a geotiff which is a format that can be operated on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "\n",
    "1. NetCDF file\n",
    "2. (TEMPORAL) sub_index or full index (specify index to loop over)\n",
    "3. etccdi index ex(tx10pETCCDI)\n",
    "\n",
    "\n",
    "ADF -- Decision to just save to ONE WORKING raster that will continously be rewritten \n",
    "rationale: The purpose of having unique tifs is to visualize holes in the data. However, this is not worth the space. If holes appear in the tabular dataset, a new geotiff corresponding to that month / year can quickly be produced!\n",
    "\n",
    "ADF -- Rationalize why this is best:\n",
    "    # Resample the raster data to the new resolution\n",
    "    resampled_raster = raster_data.rio.reproject(\n",
    "        raster_data.rio.crs,\n",
    "        shape=(\n",
    "            int(raster_data.shape[1] * 10),  # Increase number of rows by a factor of 10\n",
    "            int(raster_data.shape[2] * 10)   # Increase number of columns by a factor of 10\n",
    "        ),\n",
    "        resampling=Resampling.bilinear  # Use the correct resampling method\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:\n",
    "\n",
    "- time_index_list,\n",
    "- netcdf, climate_index, \n",
    "- shapefile_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Option 1:\n",
    "\n",
    "This code generates temporal statistics for a specified climate index by iterating over selected time indices in a NetCDF file. For each time index, it reads climate data, handles data types, renames spatial dimensions, and ensures the correct CRS is set. The data is saved as both an original raster and a modified version with null values set to -9999. Using bilinear interpolation, the code then upscales the raster data by a specified factor and saves this upsampled raster. The upsampled raster is used to compute zonal statistics over regions defined in a shapefile, and these statistics are stored in a GeoDataFrame. Each resulting GeoDataFrame, containing the mean climate index values per region, is appended to a list. After processing all time indices, the code concatenates the list into a final DataFrame, which is saved as a CSV file in the specified output folder. Optional plots display the data at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Option 2:\n",
    "\n",
    "This code processes climate index data from a NetCDF file and extracts raster values for specific geographic regions over multiple time indices. It starts by loading a shapefile of geographic zones and calculates centroids for each polygonal region. For each time index, it reads climate data corresponding to a specific parameter, converting it to days if necessary, and setting spatial dimensions and CRS if undefined. The raster data is temporarily saved as a GeoTIFF file, and the point_query function is used to sample raster values at the centroids of each zone. These sampled values are added to the shapefile's GeoDataFrame and stored in a list for each time index. After the loop, all GeoDataFrames are concatenated into a single DataFrame (final_gdf), which is saved as a CSV file. The script also includes optional plotting of each time slice and closes the NetCDF file when finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding on an recurring issue:\n",
    "\n",
    "Resampling and Masking\n",
    "When you resample a raster, you're creating a new grid with finer or coarser resolution based on the original raster. The new grid’s cells are re-sized compared to the original cells. For example, if you resample a raster by a factor of 3, the original cells are subdivided into 9 smaller cells (in a 3x3 grid).\n",
    "\n",
    "A mask is a boolean grid (same resolution as the raster) used to filter out or mark specific areas of interest. This mask typically identifies areas of the raster with valid data (True) or invalid data (False, or NaN, in case of missing data). When resampling, the mask should align with the resampled raster so that only valid cells are used for resampling operations, and invalid cells remain excluded.\n",
    "\n",
    "However, issues arise if the mask is derived from the original grid, and you resample that original grid. Here's why:\n",
    "\n",
    "The Root Cause of Misalignment\n",
    "Misalignment of Grids:\n",
    "After resampling, the new raster's resolution is finer, which means you now have smaller cells within the same bounding box. But since you're using the original raster's mask (which was designed for the coarser grid), this mask will no longer fit exactly with the new raster. The finer resolution raster has different boundaries, so the mask may not align correctly to the finer grid cells.\n",
    "In other words:\n",
    "\n",
    "The original mask may cover a large region (since it was based on the coarser grid cells).\n",
    "The new resampled raster will now have many more cells that represent the same area, but the mask won't scale automatically to match the new resolution.\n",
    "Valid Mask Misalignment: The original raster's valid mask might indicate areas of valid data and invalid data at a coarse resolution, but once you resample, the mask needs to be resampled in the same way as the data. This allows the valid/invalid distinction to remain accurate for each finer cell.\n",
    "However, if you don't reapply the valid mask to the resampled raster, or if the resampling of the mask is not done properly, you'll end up with an incorrect alignment between the valid and invalid areas. This can result in several issues:\n",
    "\n",
    "The resampling process might use invalid data (or leave gaps in data) in areas where the mask says the data should be valid.\n",
    "The finer grid might end up with spatial misalignment where valid areas on the original grid are incorrectly marked as invalid (or vice versa) on the finer grid.\n",
    "More Specifically:\n",
    "Resampling Method and Masking: When you perform resampling, the method you choose (e.g., bilinear, cubic) may try to interpolate the values of neighboring cells. If the original cells have null values or missing data, the resampling method may propagate these null values into the newly resampled cells, unless you specifically exclude them from the interpolation process. This could lead to introducing new null areas in the resampled raster.\n",
    "For example, if you have a coarser raster with some missing data, and you choose bilinear interpolation for resampling, the new finer cells may get assigned interpolated values from neighboring cells that had data. But if those neighboring cells also have missing data, this could result in interpolated null values, leading to a larger area of missing data than you originally had.\n",
    "When the Mask Doesn't Align: Consider this scenario:\n",
    "The original raster has cells with valid data, but those valid cells may not exactly match the boundaries of the finer resampled raster.\n",
    "If you simply apply the original valid mask (from the coarser raster) to the finer resampled raster without rescaling or adjusting the mask, you could misalign areas of valid data and invalid data.\n",
    "In practice:\n",
    "\n",
    "If the original mask had valid data in one coarse cell, after resampling, that coarse cell becomes nine smaller cells. Some of those smaller cells might fall into areas where the mask from the coarser raster still marks them as invalid (because the original mask was larger and not rescaled).\n",
    "The new cells in the resampled raster should be individually checked for validity. A resampled mask ensures that only the valid data is used for interpolation and no new invalid data is introduced.\n",
    "A Visual Example: Let's say your original raster has a grid with one valid cell, and the surrounding cells are null (missing). After resampling:\n",
    "Without a rescaled mask, some of the new finer grid cells could be treated as valid, even though the original data was null.\n",
    "With the correct rescaled mask, the valid area is preserved in the resampled grid, and only valid data is used to fill the finer cells.\n",
    "Conclusion:\n",
    "The problem occurs because you need to resample both the data and the mask at the same time. If the mask isn't rescaled to match the new finer grid, areas that should be marked as invalid might get wrongly marked as valid, leading to interpolation errors. These errors may introduce new null values where they didn’t exist in the original raster, because the resampling method tries to fill in gaps based on a misaligned mask.\n",
    "To avoid this:\n",
    "\n",
    "Ensure that both the data and the mask are resampled with the same grid size.\n",
    "Resample the mask correctly to the finer resolution, maintaining the valid and invalid regions, so that the resampling of the data follows the mask boundaries exactly.\n",
    "If interpolation is involved, make sure to avoid interpolating over null values to prevent the introduction of invalid data.\n",
    "In Summary:\n",
    "Misalignment occurs when the resampling process changes the raster's resolution but fails to correctly adjust the associated valid mask. When the mask isn't resampled, you risk introducing new null areas or misclassifying valid/invalid cells. This can create errors in downstream processes, such as zonal statistics or further analysis, where invalid data is improperly considered valid or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes to make:\n",
    "\n",
    "for the shapefile (extent) --- the parameter here is LOOKING for a shapefile in a specific folder. Specifically in the data/processed folder. It will automatically look for whatever the PRIOGRID.shp file is. However, if the PSI team or some other research endeavor has their own fishnet to apply, that shapefile should also be located in the folder. Therefore, the input here DOES NOT need to be filled if it is anything OTHER THAN the standard PG extent. if it is, the parameter should simply be set as the name + extension (example: pgm_viewser_extent.shp)\n",
    "\n",
    "- set the folder parameter as a standard (so that it does not need to be typed in!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_etccdi_temporal_tables__centroid(index_list, etccdi, etccdi_index, '/Users/gbenz/Downloads/pg_extent/pgm_viewser_extent.shp', '/Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_etccdi = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/cddETCCDI_140_143__centroid_process.csv')\n",
    "print(validate_etccdi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_etccdi_temporal_tables(index_list, etccdi, etccdi_index, '/Users/gbenz/Downloads/pg_extent/pgm_viewser_extent.shp', '/Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_etccdi = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/cddETCCDI_140_143__centroid_process.csv')\n",
    "print(validate_etccdi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review:\n",
    "\n",
    "#### Validate completeness of the output index at PG resolution:\n",
    "\n",
    "Temporally: check that all total number of time periods match\n",
    "- Spatially: Full extent of PG (for each temporal unit!)\n",
    "\n",
    "How to do this:\n",
    "\n",
    "1. Load the 'compiled' etccdi index .csv\n",
    "2. check for null values\n",
    "\n",
    "Total length should be: X\n",
    "\n",
    "check length of temporal units should be: X (dependent on input parameters)\n",
    "check length of spatial units should be: X\n",
    "\n",
    "3. Plot the data\n",
    "\n",
    "### What we need:\n",
    "\n",
    "A complete 'clean' dataframe to reference from VIEWSER! 11.11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_null_etccdi_values('/Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/cddETCCDI_140_143__centroid_process.csv', reference_filtered_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
