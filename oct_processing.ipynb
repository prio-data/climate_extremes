{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we have downloaded a net CDF from the Copernicus Data Store: https://cds.climate.copernicus.eu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Changes:\n",
    "\n",
    "- The complete table should be saved in the directory: `data/generated/index_table_output`\n",
    "    - The file name must include the start and end dates for easier identification.\n",
    "\n",
    "- For the functions `generate_etccdi_temporal_tables` and `generate_etccdi_temporal_tables__centroid`:\n",
    "    - Suppress the printing of each graphic iteration during execution to reduce unnecessary output.\n",
    "\n",
    "- Ensure that all generated graphics reference a single, consistent scale (legend).\n",
    "\n",
    "- Issue with validation if:\n",
    "    - partial years are set in the parameters option \n",
    "    - ex. monthly data is allowed but parameters request only part of given year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do List\n",
    "\n",
    "## dependencies file\n",
    "- This is started as requirements.tx\n",
    "    - Go through each function (.py) and pull out all dependencies \n",
    "    - compile into one simplified list\n",
    "    - consult viewser and check which are absent\n",
    "    - make list of just the absent to install before running\n",
    "\n",
    "## Decision Tree Development\n",
    "- Create a **decision tree for defensible methods** based on different applications:\n",
    "    - If performing at an admin or country scale, use method X.\n",
    "    - Address the question: \"At what scale does the utility of finer-grained PRIOgrid data diminish?\" \n",
    "    - Incorporate considerations of 'other' shapefile extents.\n",
    "\n",
    "## API Considerations\n",
    "- Investigate potential **API changes**:\n",
    "    1. Ensure the `define_request.py` script is correctly referenced.\n",
    "    2. Contact CDS to determine their API update schedule (e.g., every 6 months or yearly).\n",
    "    3. Check the **CDS API forum** for transparency on updates.\n",
    "\n",
    "## Feature Enhancements\n",
    "- Add **descriptive statistics**:\n",
    "    - Include basic summary stats for the requested indices.\n",
    "- Allow users to view country-specific data:\n",
    "    - Avoid packaging into larger datasets.\n",
    "  \n",
    "## Additional Notes\n",
    "- Add a **section in the documentation** clarifying how to migrate workflows toward ingestion smoothly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingester3.extensions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Scaffolders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import os \n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Construct PGM scaffolder\n",
    "# ------------------------\n",
    "pgm = pd.DataFrame.pgm.new_structure()\n",
    "\n",
    "pgm['month'] = pgm.m.month\n",
    "pgm['year'] = pgm.m.year\n",
    "pgm = pgm.drop(columns=['month_id', 'pgm_id'])\n",
    "\n",
    "# Add coordinate attributes from ingester3\n",
    "\n",
    "pgm['lat'] = pgm.pg.lat\n",
    "pgm['long'] = pgm.pg.lon\n",
    "\n",
    "# ------------------------\n",
    "# Construct PGY scaffolder\n",
    "# ------------------------\n",
    "pgy = pgm.drop(columns=['month', 'month_id', 'pgm_id']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.4 Scaffolders\n",
    "\n",
    "#new_pg = pd.DataFrame.pg.new_structure()\n",
    "\n",
    "pg_max = new_pg['pg_id'].max()\n",
    "pg_min = new_pg['pg_id'].min()\n",
    "\n",
    "print(pg_max)\n",
    "print(pg_min)\n",
    "\n",
    "sorted_df = new_pg.sort_values(by=\"pg_id\", ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import os \n",
    "\n",
    "new_pg = pd.DataFrame.pg.new_structure()\n",
    "\n",
    "# new_pg['month'] = new_pg.m.month\n",
    "# new_pg['year'] = new_pg.m.year\n",
    "\n",
    "# new_pg['lat'] = new_pg.pg.lat\n",
    "# new_pg['long'] = new_pg.pg.lon\n",
    "\n",
    "home_path = os.getcwd()\n",
    "\n",
    "# Define the function to create a 0.5x0.5 degree grid cell around a center point\n",
    "def create_grid_cell(lat, lon, cell_size=0.5):\n",
    "    half_size = cell_size / 2\n",
    "    # Define the corners of the grid cell\n",
    "    return Polygon([\n",
    "        (lon - half_size, lat - half_size),  # Bottom-left\n",
    "        (lon + half_size, lat - half_size),  # Bottom-right\n",
    "        (lon + half_size, lat + half_size),  # Top-right\n",
    "        (lon - half_size, lat + half_size),  # Top-left\n",
    "        (lon - half_size, lat - half_size)   # Close the polygon\n",
    "    ])\n",
    "\n",
    "# Create a GeoDataFrame with grid cells\n",
    "geometry = new_pg.apply(lambda row: create_grid_cell(row['lat'], row['long']), axis=1)\n",
    "gdf = gpd.GeoDataFrame(new_pg, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Save or visualize the grid\n",
    "\n",
    "#gdf.to_file(f\"{home_path}/data/processed/extent_shapefile/pg_viewser_extent.shp\")  # Save as a shapefile if needed\n",
    "print(gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "filtered_gdf = gdf[\n",
    "    (gdf['lat'] >= 30) & (gdf['lat'] <= 50) &\n",
    "    (gdf['long'] >= 30) & (gdf['long'] <= 50)\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot the polygons (grid cells)\n",
    "filtered_gdf.boundary.plot(ax=ax, color=\"blue\", linewidth=0.5)\n",
    "\n",
    "# Plot the center points\n",
    "filtered_gdf.plot(ax=ax, color=\"red\", markersize=5)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title(\"Grid Cells and Center Points\", fontsize=14)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pgy = new_pgy.rename(columns={'year_id':'year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pgy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pgm['month'] = new_pgm.m.month\n",
    "new_pgm['year'] = new_pgm.m.year\n",
    "\n",
    "new_pgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils path added to Python Path: /Users/gbenz/Documents/Climate_Data/climate_extremes/utils\n"
     ]
    }
   ],
   "source": [
    "from setup_environment import setup_utils_path\n",
    "setup_utils_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format API Request ----------------------------------------------------------------\n",
    "from unzip import unzip_etccdi_package\n",
    "from correct_longitude import transform_longitudinal_values\n",
    "from temporal_index import find_etccdi_timeindex, translate_index_to_daterange\n",
    "from define_request import generate_and_validate_request\n",
    "\n",
    "# Provide Metadata ------------------------------------------------------------------\n",
    "from give_metadata import give_metadata\n",
    "\n",
    "# Build API Request -----------------------------------------------------------------\n",
    "from cds_api_pull import pull_from_cds_api\n",
    "\n",
    "# Methods ---------------------------------------------------------------------------\n",
    "from etccdi_to_pg__pointquery import generate_etccdi_temporal_tables__centroid\n",
    "from etccdi_to_pg import generate_etccdi_temporal_tables\n",
    "\n",
    "# Validation ------------------------------------------------------------------------\n",
    "from give_reference_frame import provide_reference_frame\n",
    "from id_null_values import report_null_etccdi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access with Copernicus Data Store API:\n",
    "\n",
    "### You must have both a Copernicus Data Store account and have followed the proceeding instructions on setting up the CDSAPI before you can retrieve and process data from this toolbox. \n",
    "\n",
    "#### The following instructions reference the CDSAPI set up guide: https://cds.climate.copernicus.eu/how-to-api\n",
    "\n",
    "\n",
    "The final objective is to construct a main.py function that accepts the parameters below and automatically computes the process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request and Processing Parameters\n",
    "\n",
    "### API Request Parameters\n",
    "These parameters define the data retrieved via the API request:\n",
    "- **`variable`**: Specifies the data variable of interest.\n",
    "- **`product_type`**: Indicates the base product type.\n",
    "- **`experiment`**: Defines the experimental setup or scenario.\n",
    "- **`temporal_aggregation`**: Determines the temporal resolution of the data.\n",
    "\n",
    "### Processing Parameters\n",
    "These parameters guide the processing workflow:\n",
    "- **`start_year`**: `'1995'`\n",
    "- **`start_month`**: `'01'`\n",
    "- **`end_year`**: `'2000'`\n",
    "- **`end_month`**: `'12'`\n",
    "\n",
    "### Method Selection\n",
    "Choose a resampling or data alignment method:\n",
    "- **Bilinear sampling**: Applies weighted interpolation for smoother results.\n",
    "- **Point neighbor**: Maps the closest neighboring value preserving the original coarse resolution.\n",
    "\n",
    "### (In Development) Extent Selection\n",
    "Additional options for spatial analysis:\n",
    "- Empirical distributions based on basin-specific extents.\n",
    "- Future iterations will enable a 'hotspot' methodology for comparison with global metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of CDS Request\n",
    "\n",
    "```\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "request = {\n",
    "    \"variable\": [\"cold_days\"], \n",
    "    \"product_type\": [\"base_period_1961_1990\"],\n",
    "    \"model\": [\"hadgem3_gc31_ll\"],\n",
    "    \"ensemble_member\": [\"r1i1p1f3\"],\n",
    "    \"experiment\": [\"ssp1_2_6\"],\n",
    "    \"temporal_aggregation\": [\"monthly\"],\n",
    "    \"period\": [\"201501_210012\"],\n",
    "    \"version\": [\"2_0\"]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download()\n",
    "```   \n",
    "``` \n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "request = {\n",
    "    \"variable\": [\"consecutive_dry_days\"],\n",
    "    \"product_type\": [\"base_independent\"],\n",
    "    \"model\": [\"hadgem3_gc31_ll\"],\n",
    "    \"ensemble_member\": [\"r1i1p1f3\"],\n",
    "    \"experiment\": [\"ssp1_2_6\"],\n",
    "    \"temporal_aggregation\": [\"yearly\"],\n",
    "    \"period\": [\"2015_2100\"],\n",
    "    \"version\": [\"2_0\"]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informing parameters\n",
    "\n",
    "This series of prompts will help constrain appropriate parameters to construct a request that matches the CDS API\n",
    "\n",
    "First, select a temporal aggregation (yearly / monthly)\n",
    "\n",
    "- if yearly:\n",
    "supply a response that reads: (all variables are available at yearly temporal resolution. Here a list of all available climate indices.)\n",
    "\n",
    "- if monthly:\n",
    "supply a response that reads: (Select indices are available at a monthly temporal resolution. Select from the following:)\n",
    "\n",
    "\n",
    "supply the climate index from the list of available variables. Please provide the name exactly as it is written in the list.\n",
    "\n",
    "- add code check (Is the variable located within the list?) \n",
    "    - if not, supply prompt to check spelling\n",
    "\n",
    "Next, select the climate 'experiment' used to process the derived climate indices. You may select from 'historical, ssp2.. sspx... and sspz\n",
    "\n",
    "- if historical AND monthly\n",
    "    - align appropriate 'period' variable\n",
    "    - etcd (applied to sspx_x and so on)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data = {\n",
    "    \"product_type\": [\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_period_1961_1990\",\n",
    "        \"base_period_1961_1990\", \"base_period_1961_1990\", \"base_period_1961_1990\"\n",
    "    ],\n",
    "    \"variable\": [\n",
    "        \"consecutive_dry_days\", \"consecutive_wet_days\", \"diurnal_temperature_range\", \"frost_days\",\n",
    "        \"growing_season_length\", \"heavy_precipitation_days\", \"ice_days\", \"maximum_1_day_precipitation\",\n",
    "        \"maximum_5_day_precipitation\", \"maximum_value_of_daily_maximum_temperature\",\n",
    "        \"minimum_value_of_daily_maximum_temperature\", \"maximum_value_of_daily_minimum_temperature\",\n",
    "        \"minimum_value_of_daily_minimum_temperature\", \"number_of_wet_days\", \"simple_daily_intensity_index\",\n",
    "        \"summer_days\", \"total_wet_day_precipitation\", \"tropical_nights\", \"very_heavy_precipitation_days\",\n",
    "        \"cold_days\", \"cold_nights\", \"warm_days\", \"warm_nights\"\n",
    "    ],\n",
    "    \"temporal_aggregation\": [\n",
    "        \"annual\", \"annual\", \"monthly or annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"monthly or annual\",\n",
    "        \"monthly or annual\", \"monthly or annual\", \"monthly or annual\", \"monthly or annual\", \"monthly or annual\",\n",
    "        \"annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"monthly or annual\", \"monthly or annual\",\n",
    "        \"monthly or annual\", \"monthly or annual\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "#first, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First, select a temporal aggregation (yearly \\ monthly)')\n",
    "\n",
    "time = 'monthly'\n",
    "\n",
    "if time == 'monthly':\n",
    "    variable_list = df.loc[df['temporal_aggregation'].str.contains('monthly or annual'), 'variable'].tolist()\n",
    "\n",
    "    print('Select indices are available at a monthly temporal resolution. Select from the following:')\n",
    "    print()\n",
    "    print(variable_list)\n",
    "\n",
    "if time == 'yearly':\n",
    "    \n",
    "    variable_list = df['variable'].tolist()\n",
    "    print('all variables are available at yearly temporal resolution. Here a list of all available climate indices.')\n",
    "    print()\n",
    "    print(variable_list)\n",
    "\n",
    "variable = 'maximum_1_day_precipitation'\n",
    "\n",
    "#define the product type:\n",
    "\n",
    "product_type = df.loc[df['variable'] == variable, 'product_type'].values[0]\n",
    "\n",
    "if variable in variable_list:\n",
    "    print(f\"'{variable}' is a valid selection.\")\n",
    "else:\n",
    "    raise ValueError(f\"'{variable}' does not in the list. Please check your spelling!\")\n",
    "\n",
    "print('Finally, select the climate experiment used to process the derived climate indices. You may select from historical, ssp2.. sspx... and sspz')\n",
    "\n",
    "user_scenario = 'historical'\n",
    "\n",
    "if user_scenario == \"historical\" and time == \"monthly\":\n",
    "    result = \"185001_201412\"\n",
    "elif user_scenario == \"historical\" and time == \"yearly\":\n",
    "    result = \"1850_2014\"\n",
    "elif user_scenario in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"] and time == \"monthly\":\n",
    "    result = \"201501_210012\"\n",
    "elif user_scenario in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"] and time == \"yearly\":\n",
    "    result = \"2015_2100\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid combination of scenario '{user_scenario}' and time '{time}'.\")\n",
    "\n",
    "\n",
    "# Split the string by the underscore\n",
    "split_string = result.split('_') \n",
    "\n",
    "# Extract the first four characters of each element and convert them to integers\n",
    "min_value = int(split_string[0][:4])  # First four characters, converted to integer\n",
    "max_value = int(split_string[1][:4])  # First four characters, converted to integer\n",
    "\n",
    "# Define the user input (this is an example; replace with actual user input)\n",
    "user_input = \"1900\"  # This should be a string input, like from an input prompt\n",
    "\n",
    "# Convert the user input to an integer\n",
    "input_value = int(user_input)\n",
    "\n",
    "# Check if the input is within the range\n",
    "if min_value <= input_value <= max_value:\n",
    "    print(f\"The input value {input_value} is within the range {min_value}-{max_value}.\")\n",
    "else:\n",
    "    print(f\"The input value {input_value} is out of the permitted range {min_value}-{max_value}.\")\n",
    "\n",
    "print(f'acceptable years for the selected Climate Scenario ({user_scenario}) are {min_value}-{max_value}')\n",
    "#Check if temporal inputs are valid! \n",
    "#    appropriate range is defined by 'result'\n",
    "\n",
    "#is the start year within appropriate range?\n",
    "\n",
    "#is the end year within appropriate range?\n",
    "\n",
    "# Start a loop to repeatedly ask for input until a valid value is entered\n",
    "while True:\n",
    "    # Define the user input (this will be from an input prompt)\n",
    "    user_input = input(f\"Please enter a value between {min_value} and {max_value}: \")\n",
    "    \n",
    "    try:\n",
    "        # Convert the user input to an integer\n",
    "        input_value = int(user_input)\n",
    "\n",
    "        # Check if the input is within the range\n",
    "        if min_value <= input_value <= max_value:\n",
    "            print(f\"The input value {input_value} is within the range {min_value}-{max_value}.\")\n",
    "            break  # Exit the loop if the input is valid\n",
    "        else:\n",
    "            print(f\"The input value {input_value} is out of the permitted range {min_value}-{max_value}. Please try again.\")\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a valid number.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Type and Variable Temporal Aggregation\n",
    "\n",
    "| product_type            | variable                           | temporal_aggregation |\n",
    "|-------------------------|------------------------------------|----------------------|\n",
    "| base_independent         | consecutive_dry_days               | annual               |\n",
    "| base_independent         | consecutive_wet_days               | annual               |\n",
    "| base_independent         | diurnal_temperature_range          | monthly or annual    |\n",
    "| base_independent         | frost_days                         | annual               |\n",
    "| base_independent         | growing_season_length              | annual               |\n",
    "| base_independent         | heavy_precipitation_days           | annual               |\n",
    "| base_independent         | ice_days                           | annual               |\n",
    "| base_independent         | maximum_1_day_precipitation        | monthly or annual    |\n",
    "| base_independent         | maximum_5_day_precipitation        | monthly or annual    |\n",
    "| base_independent         | maximum_value_of_daily_maximum_temperature | monthly or annual    |\n",
    "| base_independent         | minimum_value_of_daily_maximum_temperature | monthly or annual    |\n",
    "| base_independent         | maximum_value_of_daily_minimum_temperature | monthly or annual    |\n",
    "| base_independent         | minimum_value_of_daily_minimum_temperature | monthly or annual    |\n",
    "| base_independent         | number_of_wet_days                 | annual               |\n",
    "| base_independent         | simple_daily_intensity_index       | annual               |\n",
    "| base_independent         | summer_days                        | annual               |\n",
    "| base_independent         | total_wet_day_precipitation        | annual               |\n",
    "| base_independent         | tropical_nights                    | annual               |\n",
    "| base_independent         | very_heavy_precipitation_days      | annual               |\n",
    "| base_period_1961_1990    | cold_days                          | monthly or annual    |\n",
    "| base_period_1961_1990    | cold_nights                        | monthly or annual    |\n",
    "| base_period_1961_1990    | warm_days                          | monthly or annual    |\n",
    "| base_period_1961_1990    | warm_nights                        | monthly or annual    |\n",
    "\n",
    "The table above defines the `product_type`, `variable`, and the corresponding `temporal_aggregation` classification options for required parameters. Variables that are only available on a yearly basis are labeled as \"annual,\" while others that can be reported on both monthly and annual bases are labeled as \"monthly or annual.\"\n",
    "\n",
    "### Experiment Period Mapping\n",
    "\n",
    "| experiment  | period        |\n",
    "|-------------|---------------|\n",
    "| historical | 185001_201412 |\n",
    "| ssp1_2_6   | 201501_210012 |\n",
    "| ssp2_4_5   | 201501_210012 |\n",
    "| ssp5_8_5   | 201501_210012 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from the .txt file\n",
    "import os\n",
    "base_dir= os.getcwd()\n",
    "config_file_path = f'{base_dir}/request.txt'  # Adjust this path to where your .txt file is located\n",
    "\n",
    "config = {}\n",
    "with open(config_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        config[key.strip()] = value.strip()\n",
    "\n",
    "# Assign variables from the config dictionary\n",
    "p_variable = config.get('p_variable')\n",
    "p_product_type = config.get('p_product_type')\n",
    "p_experiment = config.get('p_experiment')\n",
    "p_temporal_aggregation = config.get('p_temporal_aggregation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_variable = \"warm_nights\"\n",
    "p_product_type=\"base_period_1961_1990\"\n",
    "p_experiment=\"historical\"\n",
    "p_temporal_aggregation=\"yearly\"\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Define Start Year & Month\n",
    "#-----------------------------------------------------------\n",
    "start_year = '1990'\n",
    "start_month = '01'\n",
    "#-----------------------------------------------------------\n",
    "# Define End Year & Month\n",
    "end_year = '1991'\n",
    "end_month = '04'\n",
    "#-----------------------------------------------------------\n",
    "method = 'raster_query' # or raster_query\n",
    "#-----------------------------------------------------------\n",
    "save_tif = 'no' # or no\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "#country_selection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request is valid.\n",
      "yearly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'variable': ['warm_nights'],\n",
       " 'product_type': ['base_period_1961_1990'],\n",
       " 'model': ['hadgem3_gc31_ll'],\n",
       " 'ensemble_member': ['r1i1p1f3'],\n",
       " 'experiment': ['historical'],\n",
       " 'temporal_aggregation': ['yearly'],\n",
       " 'period': ['1850_2014'],\n",
       " 'version': ['2_0'],\n",
       " 'data_format': 'netcdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, calling the function will generate and validate the request\n",
    "\n",
    "request = generate_and_validate_request(\n",
    "    variable=p_variable,\n",
    "    product_type=p_product_type,\n",
    "    experiment=p_experiment,\n",
    "    temporal_aggregation=p_temporal_aggregation\n",
    ")\n",
    "temporal_aggregation_value = request['temporal_aggregation'][0]\n",
    "print(temporal_aggregation_value)\n",
    "display(request)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Load a clean PG dataframe at a consistent temporal resolution\n",
    "# to the request built\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#reference_df = provide_reference_frame(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip_file_name will appear in folder:\n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── raw_external/\n",
    "│   │   └── cds_zip/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 13:07:19,353 WARNING [2024-11-27T00:00:00] System is back on service under testing conditions. Please follow updates [here](https://forum.ecmwf.int/t/cds-ads-and-ewds-down-until-further-notice/8015) and status [here](https://status.ecmwf.int/)\n",
      "2024-11-30 13:07:19,355 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-30 13:07:19,356 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-30 13:07:19,356 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-30 13:07:19,357 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-30 13:07:19,592 INFO Request ID is b4d04896-a88d-4676-a0e5-97a566957bdf\n",
      "2024-11-30 13:07:19,744 INFO status has been updated to accepted\n",
      "2024-11-30 13:07:23,341 INFO status has been updated to running\n",
      "2024-11-30 13:07:25,797 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a222fb71d36d486d9a017ad0d0860acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b02b9d4ea6e41fe755bf272d57e1813a.zip:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zip_file_name = pull_from_cds_api(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The netcdf file will appear in folder: \n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── netcd/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn90pETCCDI\n",
      "Extracted file names: tn90pETCCDI_yr_HadGEM3-GC31-LL_historical_r1i1p1f3_b1961-1990_v20190624_1850-2014_v2-0.nc\n"
     ]
    }
   ],
   "source": [
    "netcdf_file, etccdi_index = unzip_etccdi_package(zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted netcdf file replaced the original file in location:\n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── netcd/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable 'tn90pETCCDI' was found in the file path and the world continues to spin.\n",
      "Original Latitude range: -89.375 to 89.375\n",
      "Original Longitude range: 0.9375 to 359.0625\n",
      "Adjusted Longitude range: -179.0625 to 179.0625\n",
      "Adjusted dataset saved to: /Users/gbenz/Documents/Climate_Data/climate_extremes/data/generated/netcdf/adjusted_tn90pETCCDI_yr_HadGEM3-GC31-LL_historical_r1i1p1f3_b1961-1990_v20190624_1850-2014_v2-0.nc.nc\n"
     ]
    }
   ],
   "source": [
    "etccdi = transform_longitudinal_values(etccdi_index, netcdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude range: -89.375 to 89.375\n",
      "Longitude range: -179.0625 to 179.0625\n",
      "Latitude resolution: 1.25\n",
      "Longitude resolution: 1.875\n",
      "Global Metadata:\n",
      "CDI: Climate Data Interface version 1.8.0 (http://mpimet.mpg.de/cdi)\n",
      "history: Tue Nov 24 08:58:40 2020: cdo mergetime tasmax_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_18500101-19491230.nc tasmax_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_19500101-20141230.nc ./merged/tasmax_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_18500101-20141230.nc\n",
      "2019-06-19T11:16:23Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.;\n",
      "2019-06-19T11:07:16Z MIP Convert v1.1.0, Python v2.7.12, Iris v1.13.0, Numpy v1.13.3, netcdftime v1.4.1.\n",
      "source: HadGEM3-GC31-LL (2016): \n",
      "aerosol: UKCA-GLOMAP-mode\n",
      "atmos: MetUM-HadGEM3-GA7.1 (N96; 192 x 144 longitude/latitude; 85 levels; top level 85 km)\n",
      "atmosChem: none\n",
      "land: JULES-HadGEM3-GL7.1\n",
      "landIce: none\n",
      "ocean: NEMO-HadGEM3-GO6.0 (eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics; 360 x 330 longitude/latitude; 75 levels; top grid cell 0-1 m)\n",
      "ocnBgchem: none\n",
      "seaIce: CICE-HadGEM3-GSI8 (eORCA1 tripolar primarily 1 deg; 360 x 330 longitude/latitude)\n",
      "institution: Met Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n",
      "Conventions: CF-1.7 CMIP-6.2\n",
      "activity_id: CMIP\n",
      "branch_method: standard\n",
      "branch_time_in_child: 0.0\n",
      "branch_time_in_parent: 0.0\n",
      "input_creation_date: 2019-06-19T14:20:08Z\n",
      "cv_version: 6.2.20.1\n",
      "data_specs_version: 01.00.29\n",
      "experiment: all-forcing simulation of the recent past\n",
      "experiment_id: historical\n",
      "external_variables: areacella\n",
      "forcing_index: 3\n",
      "input_frequency: day\n",
      "further_info_url: https://furtherinfo.es-doc.org/CMIP6.MOHC.HadGEM3-GC31-LL.historical.none.r1i1p1f3\n",
      "grid: Native N96 grid; 192 x 144 longitude/latitude\n",
      "grid_label: gn\n",
      "initialization_index: 1\n",
      "institution_id: MOHC\n",
      "mip_era: CMIP6\n",
      "mo_runid: u-bg466\n",
      "nominal_resolution: 250 km\n",
      "parent_activity_id: CMIP\n",
      "parent_experiment_id: piControl\n",
      "parent_mip_era: CMIP6\n",
      "parent_source_id: HadGEM3-GC31-LL\n",
      "parent_time_units: days since 1850-01-01-00-00-00\n",
      "parent_variant_label: r1i1p1f1\n",
      "physics_index: 1\n",
      "product: model-output\n",
      "realization_index: 1\n",
      "realm: atmos\n",
      "source_id: HadGEM3-GC31-LL\n",
      "source_type: AOGCM AER\n",
      "sub_experiment: none\n",
      "sub_experiment_id: none\n",
      "table_id: day\n",
      "table_info: Creation Date:(13 December 2018) MD5:2b12b5db6db112aa8b8b0d6c1645b121\n",
      "input_title: HadGEM3-GC31-LL output prepared for CMIP6\n",
      "variable_id: tasmax\n",
      "variant_label: r1i1p1f3\n",
      "license: CMIP6 model data produced by the Met Office Hadley Centre is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file) and at https://ukesm.ac.uk/cmip6. The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.\n",
      "cmor_version: 3.4.0\n",
      "input_tracking_id: hdl:21.14100/ce656162-9898-47ca-be4f-ef0dec151bfc\n",
      "CDO: Climate Data Operators version 1.8.0 (http://mpimet.mpg.de/cdo)\n",
      "ETCCDI_institution: Center for International Climate and Environmental Research - Oslo, Norway\n",
      "ETCCDI_institution_id: CICERO\n",
      "ETCCDI_software: climdex.pcic\n",
      "ETCCDI_software_version: 1.1.9.1\n",
      "frequency: yr\n",
      "creation_date: 2020-11-24T14:35:50Z\n",
      "title: ETCCDI indices computed on HadGEM3-GC31-LL output prepared for CMIP6\n",
      "Unique Years: [1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]\n",
      "Unique Months: [6]\n"
     ]
    }
   ],
   "source": [
    "give_metadata(etccdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DataFrame' has no attribute 'pg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbenz/Documents/Climate_Data/climate_extremes/oct_processing.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate_Data/climate_extremes/oct_processing.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m reference_df \u001b[39m=\u001b[39m provide_reference_frame(\u001b[39m'\u001b[39;49m\u001b[39myearly\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Climate_Data/climate_extremes/utils/give_reference_frame.py:40\u001b[0m, in \u001b[0;36mprovide_reference_frame\u001b[0;34m(temporal_aggregation_value)\u001b[0m\n\u001b[1;32m     31\u001b[0m ref_shapefile_path \u001b[39m=\u001b[39m project_root \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mprocessed\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mextent_shapefile\u001b[39m\u001b[39m'\u001b[39m \n\u001b[1;32m     33\u001b[0m \u001b[39m#temporal_aggregation_value = param_request['temporal_aggregation'][0]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Construct PG scaffolder\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# ------------------------\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m pg \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mpg\u001b[39m.\u001b[39mnew_structure()\n\u001b[1;32m     42\u001b[0m pg[\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pg\u001b[39m.\u001b[39mpg\u001b[39m.\u001b[39mlat\n\u001b[1;32m     43\u001b[0m pg[\u001b[39m'\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pg\u001b[39m.\u001b[39mpg\u001b[39m.\u001b[39mlon\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DataFrame' has no attribute 'pg'"
     ]
    }
   ],
   "source": [
    "reference_df = provide_reference_frame('yearly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list, reference_filtered_time, report_temporal_dimensions = translate_index_to_daterange(etccdi, reference_df, temporal_aggregation_value, start_year, start_month, end_year, end_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:\n",
    "\n",
    "- time_index_list,\n",
    "- netcdf, climate_index, \n",
    "- shapefile_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Raster files:\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── index_raster_output/\n",
    "```\n",
    "\n",
    "Final output table:\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── index_table_output/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes:\n",
    "\n",
    "#### Do not print each iteration of the graphics!\n",
    "- Complete for raster_query\n",
    "- Finish for resample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.enums import Resampling\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import math  # Add this import at the top of your file\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages  # Import PdfPages for saving PDF layouts\n",
    "import tempfile\n",
    "\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "def generate_layout_and_save(param_time_index_list, plot_figures, output_folder, param_climate_index):\n",
    "    columns = 4\n",
    "    rows = 3\n",
    "    plots_per_page = columns * rows\n",
    "    total_plots = len(plot_figures)\n",
    "    total_pages = math.ceil(total_plots / plots_per_page)\n",
    "\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_folder / f'{param_climate_index}_layout__resample.pdf'\n",
    "\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            fig, axes = plt.subplots(rows, columns, figsize=(11.69, 8.27))  # A4 size in landscape\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i in range(plots_per_page):\n",
    "                plot_index = page * plots_per_page + i\n",
    "                if plot_index < total_plots:\n",
    "                    fig_plot = plot_figures[plot_index]\n",
    "                    \n",
    "                    # Remove x and y labels, but keep the axes and legend\n",
    "                    for ax in fig_plot.get_axes():\n",
    "                        ax.set_xlabel('')  # Remove x-axis label\n",
    "                        ax.set_ylabel('')  # Remove y-axis label\n",
    "\n",
    "                    # Save each figure to a temporary file, then load it\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmpfile:\n",
    "                        fig_plot.savefig(tmpfile.name, bbox_inches='tight')  # Save plot with legend\n",
    "                        img = plt.imread(tmpfile.name)\n",
    "                        axes[i].imshow(img)  # Place the image into the subplot axis\n",
    "                        axes[i].axis('off')  # Turn off axis for a cleaner layout\n",
    "                else:\n",
    "                    axes[i].axis('off')  # Hide unused subplots\n",
    "\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"All graphics saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def generate_etccdi_temporal_tables(param_time_index_list, param_netcdf, param_climate_index, temporal_params, save_raster, param_shapefile_name='pgm_viewser_extent.shp'):\n",
    "    project_root = Path.cwd()  # Set this to your project root manually if needed\n",
    "\n",
    "    map_folder = project_root / 'docs' / 'Graphics' / 'Standard_review'\n",
    "\n",
    "    extent_path = project_root / 'data' / 'processed' / 'extent_shapefile'\n",
    "    extent_filename = extent_path / param_shapefile_name\n",
    "\n",
    "\n",
    "    out_originalraster_folder = project_root / 'data' / 'generated' / 'index_raster_output' /'native' \n",
    "    out_upsampleraster_folder = project_root / 'data' / 'generated' / 'index_raster_output' / 'method' / 'upsampled'\n",
    "\n",
    "    generated_index_table_folder = project_root / 'data' / 'generated' / 'index_table_output'\n",
    "\n",
    "    temporal_attribution = '_'.join(temporal_params)\n",
    "\n",
    "    all_stats = []\n",
    "    plot_figures = []  # Initialize list to store figures\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    #os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Retrieve the first and last time indices for file naming\n",
    "    first_time_index = param_time_index_list[0]\n",
    "    last_time_index = param_time_index_list[-1]\n",
    "\n",
    "    for i in param_time_index_list:\n",
    "        print(f\"Processing time index: {i}\")\n",
    "        \n",
    "        # Select the data for the specified climate index\n",
    "        data = param_netcdf[param_climate_index]\n",
    "        \n",
    "        # Check the data type and process accordingly\n",
    "        data_type = data.dtype\n",
    "        if data_type == 'timedelta64[ns]':\n",
    "            data_days = data / np.timedelta64(1, 'D')  # Convert to days if it's timedelta\n",
    "            raster_data = data_days.isel(time=i)\n",
    "        elif data_type == 'float32':\n",
    "            raster_data = data.isel(time=i)  # Use as-is if it's already float32\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type '{data_type}' for variable '{param_climate_index}'. Expected 'timedelta64[ns]' or 'float32'.\")\n",
    "        \n",
    "        # Convert spatial dimensions\n",
    "        raster_data = raster_data.rename({'lon': 'x', 'lat': 'y'})\n",
    "        raster_data = raster_data.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "\n",
    "        # Get the date and time information\n",
    "        date_time = str(param_netcdf['time'].isel(time=i).values.item())\n",
    "        year, month = date_time.split('-')[:2]\n",
    "        print(\"Year:\", year, \"Month:\", month)\n",
    "\n",
    "        # Set CRS if not already defined\n",
    "        if not raster_data.rio.crs:\n",
    "            print(\"CRS is not set. Setting CRS to EPSG:4326\")\n",
    "            raster_data = raster_data.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Save the original raster to the designated folder\n",
    "        # --- original_raster_path = os.path.join(out_originalraster_folder, f\"original_{param_climate_index}_{year}_{month}.tif\")\n",
    "        # ---- raster_data.rio.to_raster(original_raster_path)\n",
    "        # ---- print(f\"Original raster saved at: {original_raster_path}\")\n",
    "\n",
    "        # Create a separate raster with null values set to -9999\n",
    "        # --- raster_with_nulls_set = raster_data.fillna(-9999)\n",
    "\n",
    "        # Save the modified raster (with nulls as -9999) to the designated folder\n",
    "        # --- null_set_raster_path = os.path.join(out_originalraster_folder, f\"null_set_{param_climate_index}_{year}_{month}.tif\")\n",
    "        # --- raster_with_nulls_set.rio.to_raster(null_set_raster_path)\n",
    "        # --- print(f\"Raster with nulls set to -9999 saved at: {null_set_raster_path}\")\n",
    "\n",
    "        # Resample directly with bilinear interpolation\n",
    "        def resample_with_bilinear(raster_data, factor=3):\n",
    "            # Resample with the target shape\n",
    "            upsampled_raster = raster_data.rio.reproject(\n",
    "                raster_data.rio.crs,\n",
    "                shape=(\n",
    "                    int(raster_data.sizes['y'] * factor),\n",
    "                    int(raster_data.sizes['x'] * factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            return upsampled_raster\n",
    "\n",
    "        # Apply the resampling method\n",
    "        upsampled_raster = resample_with_bilinear(raster_data, factor=20)\n",
    "\n",
    "        # Plot the resampled raster\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        upsampled_raster.plot(cmap='viridis')\n",
    "        plt.title(f'Upsampled Raster for {param_climate_index} at Time Index {i}')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        #plt.show()\n",
    "\n",
    "        if save_raster == 'yes':\n",
    "            upsampled_raster_path = os.path.join(out_upsampleraster_folder, f\"upsampled_{param_climate_index}_{year}_{month}.tif\")\n",
    "            upsampled_raster.rio.to_raster(upsampled_raster_path)\n",
    "            print(f\"Upsampled raster saved at: {upsampled_raster_path}\")\n",
    "        else:\n",
    "        # Save the resampled raster to the designated folder\n",
    "            with MemoryFile() as memfile:\n",
    "                with memfile.open(driver='GTiff', \n",
    "                                width=upsampled_raster.rio.width, \n",
    "                                height=upsampled_raster.rio.height, \n",
    "                                count=1, \n",
    "                                dtype=upsampled_raster.dtype, \n",
    "                                crs=upsampled_raster.rio.crs, \n",
    "                                transform=upsampled_raster.rio.transform()) as dataset:\n",
    "                    dataset.write(upsampled_raster.values, 1)\n",
    "\n",
    "                # Load the shapefile for zonal statistics\n",
    "                gdf = gpd.read_file(extent_filename)\n",
    "                gdf = gdf[['gid', 'geometry', 'xcoord', 'ycoord']]\n",
    "\n",
    "                # Calculate zonal statistics on the upsampled raster\n",
    "                stats = zonal_stats(gdf, memfile, stats='mean', geojson_out=True)\n",
    "                stats_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "                # Add year and month fields\n",
    "                stats_gdf['year'] = year\n",
    "                stats_gdf['month'] = month\n",
    "                stats_gdf.rename(columns={'mean': param_climate_index}, inplace=True)\n",
    "\n",
    "                # Ensure stats_gdf has valid geometry and data\n",
    "                stats_gdf = stats_gdf[stats_gdf.geometry.notnull() & stats_gdf[param_climate_index].notnull()]\n",
    "            del upsampled_raster  # Clean up if no longer needed\n",
    "\n",
    "        # Plot the zonal statistics if there is data\n",
    "        if not stats_gdf.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            stats_gdf.plot(column=param_climate_index, ax=ax, legend=True, cmap='viridis', edgecolor='none')\n",
    "            ax.set_title(f'{param_climate_index} Statistics by Region - {year}-{month}')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "            #plt.show()\n",
    "            plot_figures.append(fig)  # Append figure to list\n",
    "            plt.close(fig)\n",
    "\n",
    "        else:\n",
    "            print(f\"No valid zonal statistics data to plot for {param_climate_index} at time index {i}\")\n",
    "\n",
    "        # Append to list\n",
    "        all_stats.append(stats_gdf)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    final_gdf = pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "    # Save final DataFrame to CSV in the designated folder\n",
    "\n",
    "    file_name = f\"{param_climate_index}_{temporal_attribution}__centroid_process.csv\"\n",
    "    output_file_path = generated_index_table_folder / file_name\n",
    "\n",
    "\n",
    "    final_gdf.to_csv(output_file_path, index=False)\n",
    "    print(f\"Final DataFrame saved to: {output_file_path}\")\n",
    "\n",
    "    generate_layout_and_save(param_time_index_list, plot_figures, map_folder, param_climate_index)\n",
    "\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'raster_query':\n",
    "    translated_filename = generate_etccdi_temporal_tables__centroid(index_list, etccdi, etccdi_index, report_temporal_dimensions, save_tif)\n",
    "\n",
    "elif method == 'resample':\n",
    "    translated_filename = generate_etccdi_temporal_tables(index_list, etccdi, etccdi_index, report_temporal_dimensions, save_tif)\n",
    "\n",
    "else: \n",
    "    print('you have entered a bad prompt for the method parameter. Please restart.... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``translated_filename`` retrieves the summary table saved to:\n",
    "\n",
    "\n",
    "``reference_filtered_time`` retrieves the primary reference table saved to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There was an error in report_null_etccdi_values:\n",
    "proceeding code line concentrates on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = str(translated_filename).split('_')[0]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_null_etccdi_values(translated_filename, reference_filtered_time, temporal_aggregation_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you can migrate to the ingestion script\n",
    "\n",
    "We want to intentionally keep these things seperate (lock / key) so unwanted things are not automatically ingested\n",
    "\n",
    "- **Clarify migration to the ingestion script**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Now you are ready to run the ingestion code located in .... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "from ingester3.config import source_db_path\n",
    "\n",
    "engine = sa.create_engine(source_db_path)\n",
    "gdf_pid = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT id as priogrid_gid, in_africa, in_me, geom FROM prod.priogrid\", \n",
    "    engine, \n",
    "    geom_col='geom'\n",
    ")\n",
    "gdf_pid = gdf_pid.to_crs(4326)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
