{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we have downloaded a net CDF from the Copernicus Data Store: https://cds.climate.copernicus.eu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Changes:\n",
    "\n",
    "- The complete table should be saved in the directory: `data/generated/index_table_output`\n",
    "    - The file name must include the start and end dates for easier identification.\n",
    "\n",
    "- For the functions `generate_etccdi_temporal_tables` and `generate_etccdi_temporal_tables__centroid`:\n",
    "    - Suppress the printing of each graphic iteration during execution to reduce unnecessary output.\n",
    "\n",
    "- Ensure that all generated graphics reference a single, consistent scale (legend).\n",
    "\n",
    "- Issue with validation if:\n",
    "    - partial years are set in the parameters option \n",
    "    - ex. monthly data is allowed but parameters request only part of given year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do List\n",
    "\n",
    "## dependencies file\n",
    "- This is started as requirements.tx\n",
    "    - Go through each function (.py) and pull out all dependencies \n",
    "    - compile into one simplified list\n",
    "    - consult viewser and check which are absent\n",
    "    - make list of just the absent to install before running\n",
    "\n",
    "## Decision Tree Development\n",
    "- Create a **decision tree for defensible methods** based on different applications:\n",
    "    - If performing at an admin or country scale, use method X.\n",
    "    - Address the question: \"At what scale does the utility of finer-grained PRIOgrid data diminish?\" \n",
    "    - Incorporate considerations of 'other' shapefile extents.\n",
    "\n",
    "## API Considerations\n",
    "- Investigate potential **API changes**:\n",
    "    1. Ensure the `define_request.py` script is correctly referenced.\n",
    "    2. Contact CDS to determine their API update schedule (e.g., every 6 months or yearly).\n",
    "    3. Check the **CDS API forum** for transparency on updates.\n",
    "\n",
    "## Feature Enhancements\n",
    "- Add **descriptive statistics**:\n",
    "    - Include basic summary stats for the requested indices.\n",
    "- Allow users to view country-specific data:\n",
    "    - Avoid packaging into larger datasets.\n",
    "  \n",
    "## Additional Notes\n",
    "- Add a **section in the documentation** clarifying how to migrate workflows smoothly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils path already in Python Path.\n"
     ]
    }
   ],
   "source": [
    "from setup_environment import setup_utils_path\n",
    "setup_utils_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format API Request ----------------------------------------------------------------\n",
    "from unzip import unzip_etccdi_package\n",
    "from correct_longitude import transform_longitudinal_values\n",
    "from temporal_index import find_etccdi_timeindex, translate_index_to_daterange\n",
    "from define_request import generate_and_validate_request\n",
    "\n",
    "# Provide Metadata ------------------------------------------------------------------\n",
    "from give_metadata import give_metadata\n",
    "\n",
    "# Build API Request -----------------------------------------------------------------\n",
    "from cds_api_pull import pull_from_cds_api\n",
    "\n",
    "# Methods ---------------------------------------------------------------------------\n",
    "from etccdi_to_pg__pointquery import generate_etccdi_temporal_tables__centroid\n",
    "from etccdi_to_pg import generate_etccdi_temporal_tables\n",
    "\n",
    "# Validation ------------------------------------------------------------------------\n",
    "from give_reference_frame import provide_reference_frame\n",
    "from id_null_values import report_null_etccdi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access with Copernicus Data Store API:\n",
    "\n",
    "### You must have both a Copernicus Data Store account and have followed the proceeding instructions on setting up the CDSAPI before you can retrieve and process data from this toolbox. \n",
    "\n",
    "#### The following instructions reference the CDSAPI set up guide: https://cds.climate.copernicus.eu/how-to-api\n",
    "\n",
    "\n",
    "The final objective is to construct a main.py function that accepts the parameters below and automatically computes the process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Request and Processing Parameters\n",
    "\n",
    "### API Request Parameters\n",
    "These parameters define the data retrieved via the API request:\n",
    "- **`variable`**: Specifies the data variable of interest.\n",
    "- **`product_type`**: Indicates the base product type.\n",
    "- **`experiment`**: Defines the experimental setup or scenario.\n",
    "- **`temporal_aggregation`**: Determines the temporal resolution of the data.\n",
    "\n",
    "### Processing Parameters\n",
    "These parameters guide the processing workflow:\n",
    "- **`start_year`**: `'1995'`\n",
    "- **`start_month`**: `'01'`\n",
    "- **`end_year`**: `'2000'`\n",
    "- **`end_month`**: `'12'`\n",
    "\n",
    "### Method Selection\n",
    "Choose a resampling or data alignment method:\n",
    "- **Bilinear sampling**: Applies weighted interpolation for smoother results.\n",
    "- **Point neighbor**: Maps the closest neighboring value preserving the original coarse resolution.\n",
    "\n",
    "### (In Development) Extent Selection\n",
    "Additional options for spatial analysis:\n",
    "- Empirical distributions based on basin-specific extents.\n",
    "- Future iterations will enable a 'hotspot' methodology for comparison with global metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of CDS Request\n",
    "\n",
    "```\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "request = {\n",
    "    \"variable\": [\"cold_days\"], \n",
    "    \"product_type\": [\"base_period_1961_1990\"],\n",
    "    \"model\": [\"hadgem3_gc31_ll\"],\n",
    "    \"ensemble_member\": [\"r1i1p1f3\"],\n",
    "    \"experiment\": [\"ssp1_2_6\"],\n",
    "    \"temporal_aggregation\": [\"monthly\"],\n",
    "    \"period\": [\"201501_210012\"],\n",
    "    \"version\": [\"2_0\"]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download()\n",
    "```   \n",
    "``` \n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "request = {\n",
    "    \"variable\": [\"consecutive_dry_days\"],\n",
    "    \"product_type\": [\"base_independent\"],\n",
    "    \"model\": [\"hadgem3_gc31_ll\"],\n",
    "    \"ensemble_member\": [\"r1i1p1f3\"],\n",
    "    \"experiment\": [\"ssp1_2_6\"],\n",
    "    \"temporal_aggregation\": [\"yearly\"],\n",
    "    \"period\": [\"2015_2100\"],\n",
    "    \"version\": [\"2_0\"]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informing parameters\n",
    "\n",
    "This series of prompts will help constrain appropriate parameters to construct a request that matches the CDS API\n",
    "\n",
    "First, select a temporal aggregation (yearly / monthly)\n",
    "\n",
    "- if yearly:\n",
    "supply a response that reads: (all variables are available at yearly temporal resolution. Here a list of all available climate indices.)\n",
    "\n",
    "- if monthly:\n",
    "supply a response that reads: (Select indices are available at a monthly temporal resolution. Select from the following:)\n",
    "\n",
    "\n",
    "supply the climate index from the list of available variables. Please provide the name exactly as it is written in the list.\n",
    "\n",
    "- add code check (Is the variable located within the list?) \n",
    "    - if not, supply prompt to check spelling\n",
    "\n",
    "Next, select the climate 'experiment' used to process the derived climate indices. You may select from 'historical, ssp2.. sspx... and sspz\n",
    "\n",
    "- if historical AND monthly\n",
    "    - align appropriate 'period' variable\n",
    "    - etcd (applied to sspx_x and so on)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             product_type                                    variable  \\\n",
      "0        base_independent                        consecutive_dry_days   \n",
      "1        base_independent                        consecutive_wet_days   \n",
      "2        base_independent                   diurnal_temperature_range   \n",
      "3        base_independent                                  frost_days   \n",
      "4        base_independent                       growing_season_length   \n",
      "5        base_independent                    heavy_precipitation_days   \n",
      "6        base_independent                                    ice_days   \n",
      "7        base_independent                 maximum_1_day_precipitation   \n",
      "8        base_independent                 maximum_5_day_precipitation   \n",
      "9        base_independent  maximum_value_of_daily_maximum_temperature   \n",
      "10       base_independent  minimum_value_of_daily_maximum_temperature   \n",
      "11       base_independent  maximum_value_of_daily_minimum_temperature   \n",
      "12       base_independent  minimum_value_of_daily_minimum_temperature   \n",
      "13       base_independent                          number_of_wet_days   \n",
      "14       base_independent                simple_daily_intensity_index   \n",
      "15       base_independent                                 summer_days   \n",
      "16       base_independent                 total_wet_day_precipitation   \n",
      "17       base_independent                             tropical_nights   \n",
      "18       base_independent               very_heavy_precipitation_days   \n",
      "19  base_period_1961_1990                                   cold_days   \n",
      "20  base_period_1961_1990                                 cold_nights   \n",
      "21  base_period_1961_1990                                   warm_days   \n",
      "22  base_period_1961_1990                                 warm_nights   \n",
      "\n",
      "   temporal_aggregation  \n",
      "0                annual  \n",
      "1                annual  \n",
      "2     monthly or annual  \n",
      "3                annual  \n",
      "4                annual  \n",
      "5                annual  \n",
      "6                annual  \n",
      "7     monthly or annual  \n",
      "8     monthly or annual  \n",
      "9     monthly or annual  \n",
      "10    monthly or annual  \n",
      "11    monthly or annual  \n",
      "12    monthly or annual  \n",
      "13               annual  \n",
      "14               annual  \n",
      "15               annual  \n",
      "16               annual  \n",
      "17               annual  \n",
      "18               annual  \n",
      "19    monthly or annual  \n",
      "20    monthly or annual  \n",
      "21    monthly or annual  \n",
      "22    monthly or annual  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the DataFrame\n",
    "data = {\n",
    "    \"product_type\": [\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_independent\",\n",
    "        \"base_independent\", \"base_independent\", \"base_independent\", \"base_period_1961_1990\",\n",
    "        \"base_period_1961_1990\", \"base_period_1961_1990\", \"base_period_1961_1990\"\n",
    "    ],\n",
    "    \"variable\": [\n",
    "        \"consecutive_dry_days\", \"consecutive_wet_days\", \"diurnal_temperature_range\", \"frost_days\",\n",
    "        \"growing_season_length\", \"heavy_precipitation_days\", \"ice_days\", \"maximum_1_day_precipitation\",\n",
    "        \"maximum_5_day_precipitation\", \"maximum_value_of_daily_maximum_temperature\",\n",
    "        \"minimum_value_of_daily_maximum_temperature\", \"maximum_value_of_daily_minimum_temperature\",\n",
    "        \"minimum_value_of_daily_minimum_temperature\", \"number_of_wet_days\", \"simple_daily_intensity_index\",\n",
    "        \"summer_days\", \"total_wet_day_precipitation\", \"tropical_nights\", \"very_heavy_precipitation_days\",\n",
    "        \"cold_days\", \"cold_nights\", \"warm_days\", \"warm_nights\"\n",
    "    ],\n",
    "    \"temporal_aggregation\": [\n",
    "        \"annual\", \"annual\", \"monthly or annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"monthly or annual\",\n",
    "        \"monthly or annual\", \"monthly or annual\", \"monthly or annual\", \"monthly or annual\", \"monthly or annual\",\n",
    "        \"annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"annual\", \"monthly or annual\", \"monthly or annual\",\n",
    "        \"monthly or annual\", \"monthly or annual\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "#first, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, select a temporal aggregation (yearly \\ monthly)\n",
      "Select indices are available at a monthly temporal resolution. Select from the following:\n",
      "\n",
      "['diurnal_temperature_range', 'maximum_1_day_precipitation', 'maximum_5_day_precipitation', 'maximum_value_of_daily_maximum_temperature', 'minimum_value_of_daily_maximum_temperature', 'maximum_value_of_daily_minimum_temperature', 'minimum_value_of_daily_minimum_temperature', 'cold_days', 'cold_nights', 'warm_days', 'warm_nights']\n",
      "'maximum_1_day_precipitation' is a valid selection.\n",
      "Finally, select the climate experiment used to process the derived climate indices. You may select from historical, ssp2.. sspx... and sspz\n"
     ]
    }
   ],
   "source": [
    "print('First, select a temporal aggregation (yearly \\ monthly)')\n",
    "\n",
    "time = 'monthly'\n",
    "\n",
    "if time == 'monthly':\n",
    "    variable_list = df.loc[df['temporal_aggregation'].str.contains('monthly or annual'), 'variable'].tolist()\n",
    "\n",
    "    print('Select indices are available at a monthly temporal resolution. Select from the following:')\n",
    "    print()\n",
    "    print(variable_list)\n",
    "\n",
    "if time == 'yearly':\n",
    "    \n",
    "    variable_list = df['variable'].tolist()\n",
    "    print('all variables are available at yearly temporal resolution. Here a list of all available climate indices.')\n",
    "    print()\n",
    "    print(variable_list)\n",
    "\n",
    "variable = 'maximum_1_day_precipitation'\n",
    "\n",
    "#define the product type:\n",
    "\n",
    "product_type = df.loc[df['variable'] == variable, 'product_type'].values[0]\n",
    "\n",
    "if variable in variable_list:\n",
    "    print(f\"'{variable}' is a valid selection.\")\n",
    "else:\n",
    "    raise ValueError(f\"'{variable}' does not in the list. Please check your spelling!\")\n",
    "\n",
    "print('Finally, select the climate experiment used to process the derived climate indices. You may select from historical, ssp2.. sspx... and sspz')\n",
    "\n",
    "user_scenario = 'historical'\n",
    "\n",
    "if user_scenario == \"historical\" and time == \"monthly\":\n",
    "    result = \"185001_201412\"\n",
    "elif user_scenario == \"historical\" and time == \"yearly\":\n",
    "    result = \"1850_2014\"\n",
    "elif user_scenario in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"] and time == \"monthly\":\n",
    "    result = \"201501_210012\"\n",
    "elif user_scenario in [\"ssp1_2_6\", \"ssp2_4_5\", \"ssp5_8_5\"] and time == \"yearly\":\n",
    "    result = \"2015_2100\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid combination of scenario '{user_scenario}' and time '{time}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Type and Variable Temporal Aggregation\n",
    "\n",
    "| product_type            | variable                           | temporal_aggregation |\n",
    "|-------------------------|------------------------------------|----------------------|\n",
    "| base_independent         | consecutive_dry_days               | annual               |\n",
    "| base_independent         | consecutive_wet_days               | annual               |\n",
    "| base_independent         | diurnal_temperature_range          | monthly or annual    |\n",
    "| base_independent         | frost_days                         | annual               |\n",
    "| base_independent         | growing_season_length              | annual               |\n",
    "| base_independent         | heavy_precipitation_days           | annual               |\n",
    "| base_independent         | ice_days                           | annual               |\n",
    "| base_independent         | maximum_1_day_precipitation        | monthly or annual    |\n",
    "| base_independent         | maximum_5_day_precipitation        | monthly or annual    |\n",
    "| base_independent         | maximum_value_of_daily_maximum_temperature | monthly or annual    |\n",
    "| base_independent         | minimum_value_of_daily_maximum_temperature | monthly or annual    |\n",
    "| base_independent         | maximum_value_of_daily_minimum_temperature | monthly or annual    |\n",
    "| base_independent         | minimum_value_of_daily_minimum_temperature | monthly or annual    |\n",
    "| base_independent         | number_of_wet_days                 | annual               |\n",
    "| base_independent         | simple_daily_intensity_index       | annual               |\n",
    "| base_independent         | summer_days                        | annual               |\n",
    "| base_independent         | total_wet_day_precipitation        | annual               |\n",
    "| base_independent         | tropical_nights                    | annual               |\n",
    "| base_independent         | very_heavy_precipitation_days      | annual               |\n",
    "| base_period_1961_1990    | cold_days                          | monthly or annual    |\n",
    "| base_period_1961_1990    | cold_nights                        | monthly or annual    |\n",
    "| base_period_1961_1990    | warm_days                          | monthly or annual    |\n",
    "| base_period_1961_1990    | warm_nights                        | monthly or annual    |\n",
    "\n",
    "The table above defines the `product_type`, `variable`, and the corresponding `temporal_aggregation` classification options for required parameters. Variables that are only available on a yearly basis are labeled as \"annual,\" while others that can be reported on both monthly and annual bases are labeled as \"monthly or annual.\"\n",
    "\n",
    "### Experiment Period Mapping\n",
    "\n",
    "| experiment  | period        |\n",
    "|-------------|---------------|\n",
    "| historical | 185001_201412 |\n",
    "| ssp1_2_6   | 201501_210012 |\n",
    "| ssp2_4_5   | 201501_210012 |\n",
    "| ssp5_8_5   | 201501_210012 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from the .txt file\n",
    "import os\n",
    "base_dir= os.getcwd()\n",
    "config_file_path = f'{base_dir}/request.txt'  # Adjust this path to where your .txt file is located\n",
    "\n",
    "config = {}\n",
    "with open(config_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(':')\n",
    "        config[key.strip()] = value.strip()\n",
    "\n",
    "# Assign variables from the config dictionary\n",
    "p_variable = config.get('p_variable')\n",
    "p_product_type = config.get('p_product_type')\n",
    "p_experiment = config.get('p_experiment')\n",
    "p_temporal_aggregation = config.get('p_temporal_aggregation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer_days\n"
     ]
    }
   ],
   "source": [
    "print(p_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_variable = \"ice_days\"\n",
    "p_product_type=\"base_independent\"\n",
    "p_experiment=\"ssp1_2_6\"\n",
    "p_temporal_aggregation=\"yearly\"\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Define Start Year & Month\n",
    "#-----------------------------------------------------------\n",
    "start_year = '2015'\n",
    "start_month = '01'\n",
    "#-----------------------------------------------------------\n",
    "# Define End Year & Month\n",
    "end_year = '2016'\n",
    "end_month = '04'\n",
    "#-----------------------------------------------------------\n",
    "method = 'raster_query' # or raster_query\n",
    "#-----------------------------------------------------------\n",
    "save_tif = 'no' # or no\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "#country_selection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request is valid.\n",
      "yearly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'variable': ['ice_days'],\n",
       " 'product_type': ['base_independent'],\n",
       " 'model': ['hadgem3_gc31_ll'],\n",
       " 'ensemble_member': ['r1i1p1f3'],\n",
       " 'experiment': ['ssp1_2_6'],\n",
       " 'temporal_aggregation': ['yearly'],\n",
       " 'period': ['2015_2100'],\n",
       " 'version': ['2_0'],\n",
       " 'data_format': 'netcdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priogrid_gid    int64\n",
      "year            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now, calling the function will generate and validate the request\n",
    "\n",
    "request = generate_and_validate_request(\n",
    "    variable=p_variable,\n",
    "    product_type=p_product_type,\n",
    "    experiment=p_experiment,\n",
    "    temporal_aggregation=p_temporal_aggregation\n",
    ")\n",
    "temporal_aggregation_value = request['temporal_aggregation'][0]\n",
    "print(temporal_aggregation_value)\n",
    "display(request)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Load a clean PG dataframe at a consistent temporal resolution\n",
    "# to the request built\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "reference_df = provide_reference_frame(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip_file_name will appear in folder:\n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── raw_external/\n",
    "│   │   └── cds_zip/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 10:18:51,275 WARNING [2024-11-19T00:00:00] System is experiencing performance issues. Please check updated status [here](https://status.ecmwf.int/)\n",
      "2024-11-22 10:18:51,276 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-22 10:18:51,276 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-22 10:18:51,276 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-22 10:18:51,276 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-22 10:18:51,901 INFO Request ID is d1d543c2-02a2-4e67-b962-150f7aee973d\n",
      "2024-11-22 10:18:51,984 INFO status has been updated to accepted\n",
      "2024-11-22 10:18:57,125 INFO status has been updated to running\n",
      "2024-11-22 10:19:00,604 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0974e449be0542d4b0a7ea13a3118b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "d5206f4564168d3931d3d9a4f839601a.zip:   0%|          | 0.00/9.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zip_file_name = pull_from_cds_api(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The netcdf file will appear in folder: \n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── netcd/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idETCCDI\n",
      "Extracted file names: idETCCDI_yr_HadGEM3-GC31-LL_ssp126_r1i1p1f3_no-base_v20200114_2015-2100_v2-0.nc\n"
     ]
    }
   ],
   "source": [
    "netcdf_file, etccdi_index = unzip_etccdi_package(zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted netcdf file replaced the original file in location:\n",
    "\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── netcd/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable 'idETCCDI' was found in the file path and the world continues to spin.\n",
      "Original Latitude range: -89.375 to 89.375\n",
      "Original Longitude range: 0.9375 to 359.0625\n",
      "Adjusted Longitude range: -179.0625 to 179.0625\n",
      "Adjusted dataset saved to: /Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/netcdf/adjusted_idETCCDI_yr_HadGEM3-GC31-LL_ssp126_r1i1p1f3_no-base_v20200114_2015-2100_v2-0.nc.nc\n"
     ]
    }
   ],
   "source": [
    "etccdi = transform_longitudinal_values(etccdi_index, netcdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude range: -89.375 to 89.375\n",
      "Longitude range: -179.0625 to 179.0625\n",
      "Latitude resolution: 1.25\n",
      "Longitude resolution: 1.875\n",
      "Global Metadata:\n",
      "CDI: Climate Data Interface version 1.8.0 (http://mpimet.mpg.de/cdi)\n",
      "history: Tue Nov 24 09:31:53 2020: cdo mergetime tasmax_day_HadGEM3-GC31-LL_ssp126_r1i1p1f3_gn_20150101-20491230.nc tasmax_day_HadGEM3-GC31-LL_ssp126_r1i1p1f3_gn_20500101-21001230.nc ./merged/tasmax_day_HadGEM3-GC31-LL_ssp126_r1i1p1f3_gn_20150101-21001230.nc\n",
      "2020-01-13T09:55:40Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.;\n",
      "2020-01-13T09:53:59Z MIP Convert v1.2.3, Python v2.7.12, Iris v1.13.0, Numpy v1.13.3, netcdftime v1.4.1.\n",
      "source: HadGEM3-GC31-LL (2016): \n",
      "aerosol: UKCA-GLOMAP-mode\n",
      "atmos: MetUM-HadGEM3-GA7.1 (N96; 192 x 144 longitude/latitude; 85 levels; top level 85 km)\n",
      "atmosChem: none\n",
      "land: JULES-HadGEM3-GL7.1\n",
      "landIce: none\n",
      "ocean: NEMO-HadGEM3-GO6.0 (eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics; 360 x 330 longitude/latitude; 75 levels; top grid cell 0-1 m)\n",
      "ocnBgchem: none\n",
      "seaIce: CICE-HadGEM3-GSI8 (eORCA1 tripolar primarily 1 deg; 360 x 330 longitude/latitude)\n",
      "institution: Met Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n",
      "Conventions: CF-1.7 CMIP-6.2\n",
      "activity_id: ScenarioMIP\n",
      "branch_method: standard\n",
      "branch_time_in_child: 59400.0\n",
      "branch_time_in_parent: 59400.0\n",
      "input_creation_date: 2020-01-13T10:10:01Z\n",
      "cv_version: 6.2.37.5\n",
      "data_specs_version: 01.00.29\n",
      "experiment: update of RCP2.6 based on SSP1\n",
      "experiment_id: ssp126\n",
      "external_variables: areacella\n",
      "forcing_index: 3\n",
      "input_frequency: day\n",
      "further_info_url: https://furtherinfo.es-doc.org/CMIP6.MOHC.HadGEM3-GC31-LL.ssp126.none.r1i1p1f3\n",
      "grid: Native N96 grid; 192 x 144 longitude/latitude\n",
      "grid_label: gn\n",
      "initialization_index: 1\n",
      "institution_id: MOHC\n",
      "mip_era: CMIP6\n",
      "mo_runid: u-bj611\n",
      "nominal_resolution: 250 km\n",
      "parent_activity_id: CMIP\n",
      "parent_experiment_id: historical\n",
      "parent_mip_era: CMIP6\n",
      "parent_source_id: HadGEM3-GC31-LL\n",
      "parent_time_units: days since 1850-01-01\n",
      "parent_variant_label: r1i1p1f3\n",
      "physics_index: 1\n",
      "product: model-output\n",
      "realization_index: 1\n",
      "realm: atmos\n",
      "source_id: HadGEM3-GC31-LL\n",
      "source_type: AOGCM AER\n",
      "sub_experiment: none\n",
      "sub_experiment_id: none\n",
      "table_id: day\n",
      "table_info: Creation Date:(13 December 2018) MD5:f0588f7f55b5732b17302f8d9d0d7b8c\n",
      "input_title: HadGEM3-GC31-LL output prepared for CMIP6\n",
      "input_tracking_id: hdl:21.14100/8e91bf77-1c96-4585-b92c-c438a87d4135\n",
      "variable_id: tasmax\n",
      "variable_name: tasmax\n",
      "variant_label: r1i1p1f3\n",
      "license: CMIP6 model data produced by the Met Office Hadley Centre is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file) and at https://ukesm.ac.uk/cmip6. The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.\n",
      "cmor_version: 3.4.0\n",
      "CDO: Climate Data Operators version 1.8.0 (http://mpimet.mpg.de/cdo)\n",
      "ETCCDI_institution: Center for International Climate and Environmental Research - Oslo, Norway\n",
      "ETCCDI_institution_id: CICERO\n",
      "ETCCDI_software: climdex.pcic\n",
      "ETCCDI_software_version: 1.1.9.1\n",
      "frequency: yr\n",
      "creation_date: 2020-11-27T13:08:02Z\n",
      "title: ETCCDI indices computed on HadGEM3-GC31-LL output prepared for CMIP6\n",
      "Unique Years: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100]\n",
      "Unique Months: [6]\n"
     ]
    }
   ],
   "source": [
    "give_metadata(etccdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 01 of the year 2015 but located data for the first available month.\n",
      "Validation: Found data for Year: 2015, Month: 06 at index 0.\n",
      "\n",
      "No data found for 04 of the year 2016 but located data for the first available month.\n",
      "Validation: Found data for Year: 2016, Month: 06 at index 1.\n",
      "\n",
      "The start index is: 0, and references 06 (month) of 2015 (year)\n",
      "\n",
      "The end index is: 1, and references 06 (month) of 2016 (year)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_list, reference_filtered_time, report_temporal_dimensions = translate_index_to_daterange(etccdi, reference_df, temporal_aggregation_value, start_year, start_month, end_year, end_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:\n",
    "\n",
    "- time_index_list,\n",
    "- netcdf, climate_index, \n",
    "- shapefile_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Raster files:\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── index_raster_output/\n",
    "```\n",
    "\n",
    "Final output table:\n",
    "```\n",
    "CLIMATE_EXTREMES/\n",
    "├── data/\n",
    "│   ├── generated/\n",
    "│   │   └── index_table_output/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes:\n",
    "\n",
    "#### Do not print each iteration of the graphics!\n",
    "- Complete for raster_query\n",
    "- Finish for resample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.enums import Resampling\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import math  # Add this import at the top of your file\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages  # Import PdfPages for saving PDF layouts\n",
    "import tempfile\n",
    "\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "def generate_layout_and_save(param_time_index_list, plot_figures, output_folder, param_climate_index):\n",
    "    columns = 4\n",
    "    rows = 3\n",
    "    plots_per_page = columns * rows\n",
    "    total_plots = len(plot_figures)\n",
    "    total_pages = math.ceil(total_plots / plots_per_page)\n",
    "\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_folder / f'{param_climate_index}_layout__resample.pdf'\n",
    "\n",
    "    with PdfPages(output_file) as pdf:\n",
    "        for page in range(total_pages):\n",
    "            fig, axes = plt.subplots(rows, columns, figsize=(11.69, 8.27))  # A4 size in landscape\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i in range(plots_per_page):\n",
    "                plot_index = page * plots_per_page + i\n",
    "                if plot_index < total_plots:\n",
    "                    fig_plot = plot_figures[plot_index]\n",
    "                    \n",
    "                    # Remove x and y labels, but keep the axes and legend\n",
    "                    for ax in fig_plot.get_axes():\n",
    "                        ax.set_xlabel('')  # Remove x-axis label\n",
    "                        ax.set_ylabel('')  # Remove y-axis label\n",
    "\n",
    "                    # Save each figure to a temporary file, then load it\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmpfile:\n",
    "                        fig_plot.savefig(tmpfile.name, bbox_inches='tight')  # Save plot with legend\n",
    "                        img = plt.imread(tmpfile.name)\n",
    "                        axes[i].imshow(img)  # Place the image into the subplot axis\n",
    "                        axes[i].axis('off')  # Turn off axis for a cleaner layout\n",
    "                else:\n",
    "                    axes[i].axis('off')  # Hide unused subplots\n",
    "\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"All graphics saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def generate_etccdi_temporal_tables(param_time_index_list, param_netcdf, param_climate_index, temporal_params, save_raster, param_shapefile_name='pgm_viewser_extent.shp'):\n",
    "    project_root = Path.cwd()  # Set this to your project root manually if needed\n",
    "\n",
    "    map_folder = project_root / 'docs' / 'Graphics' / 'Standard_review'\n",
    "\n",
    "    extent_path = project_root / 'data' / 'processed' / 'extent_shapefile'\n",
    "    extent_filename = extent_path / param_shapefile_name\n",
    "\n",
    "\n",
    "    out_originalraster_folder = project_root / 'data' / 'generated' / 'index_raster_output' /'native' \n",
    "    out_upsampleraster_folder = project_root / 'data' / 'generated' / 'index_raster_output' / 'method' / 'upsampled'\n",
    "\n",
    "    generated_index_table_folder = project_root / 'data' / 'generated' / 'index_table_output'\n",
    "\n",
    "    temporal_attribution = '_'.join(temporal_params)\n",
    "\n",
    "    all_stats = []\n",
    "    plot_figures = []  # Initialize list to store figures\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    #os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Retrieve the first and last time indices for file naming\n",
    "    first_time_index = param_time_index_list[0]\n",
    "    last_time_index = param_time_index_list[-1]\n",
    "\n",
    "    for i in param_time_index_list:\n",
    "        print(f\"Processing time index: {i}\")\n",
    "        \n",
    "        # Select the data for the specified climate index\n",
    "        data = param_netcdf[param_climate_index]\n",
    "        \n",
    "        # Check the data type and process accordingly\n",
    "        data_type = data.dtype\n",
    "        if data_type == 'timedelta64[ns]':\n",
    "            data_days = data / np.timedelta64(1, 'D')  # Convert to days if it's timedelta\n",
    "            raster_data = data_days.isel(time=i)\n",
    "        elif data_type == 'float32':\n",
    "            raster_data = data.isel(time=i)  # Use as-is if it's already float32\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type '{data_type}' for variable '{param_climate_index}'. Expected 'timedelta64[ns]' or 'float32'.\")\n",
    "        \n",
    "        # Convert spatial dimensions\n",
    "        raster_data = raster_data.rename({'lon': 'x', 'lat': 'y'})\n",
    "        raster_data = raster_data.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "\n",
    "        # Get the date and time information\n",
    "        date_time = str(param_netcdf['time'].isel(time=i).values.item())\n",
    "        year, month = date_time.split('-')[:2]\n",
    "        print(\"Year:\", year, \"Month:\", month)\n",
    "\n",
    "        # Set CRS if not already defined\n",
    "        if not raster_data.rio.crs:\n",
    "            print(\"CRS is not set. Setting CRS to EPSG:4326\")\n",
    "            raster_data = raster_data.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Save the original raster to the designated folder\n",
    "        # --- original_raster_path = os.path.join(out_originalraster_folder, f\"original_{param_climate_index}_{year}_{month}.tif\")\n",
    "        # ---- raster_data.rio.to_raster(original_raster_path)\n",
    "        # ---- print(f\"Original raster saved at: {original_raster_path}\")\n",
    "\n",
    "        # Create a separate raster with null values set to -9999\n",
    "        # --- raster_with_nulls_set = raster_data.fillna(-9999)\n",
    "\n",
    "        # Save the modified raster (with nulls as -9999) to the designated folder\n",
    "        # --- null_set_raster_path = os.path.join(out_originalraster_folder, f\"null_set_{param_climate_index}_{year}_{month}.tif\")\n",
    "        # --- raster_with_nulls_set.rio.to_raster(null_set_raster_path)\n",
    "        # --- print(f\"Raster with nulls set to -9999 saved at: {null_set_raster_path}\")\n",
    "\n",
    "        # Resample directly with bilinear interpolation\n",
    "        def resample_with_bilinear(raster_data, factor=3):\n",
    "            # Resample with the target shape\n",
    "            upsampled_raster = raster_data.rio.reproject(\n",
    "                raster_data.rio.crs,\n",
    "                shape=(\n",
    "                    int(raster_data.sizes['y'] * factor),\n",
    "                    int(raster_data.sizes['x'] * factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            return upsampled_raster\n",
    "\n",
    "        # Apply the resampling method\n",
    "        upsampled_raster = resample_with_bilinear(raster_data, factor=20)\n",
    "\n",
    "        # Plot the resampled raster\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        upsampled_raster.plot(cmap='viridis')\n",
    "        plt.title(f'Upsampled Raster for {param_climate_index} at Time Index {i}')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        #plt.show()\n",
    "\n",
    "        if save_raster == 'yes':\n",
    "            upsampled_raster_path = os.path.join(out_upsampleraster_folder, f\"upsampled_{param_climate_index}_{year}_{month}.tif\")\n",
    "            upsampled_raster.rio.to_raster(upsampled_raster_path)\n",
    "            print(f\"Upsampled raster saved at: {upsampled_raster_path}\")\n",
    "        else:\n",
    "        # Save the resampled raster to the designated folder\n",
    "            with MemoryFile() as memfile:\n",
    "                with memfile.open(driver='GTiff', \n",
    "                                width=upsampled_raster.rio.width, \n",
    "                                height=upsampled_raster.rio.height, \n",
    "                                count=1, \n",
    "                                dtype=upsampled_raster.dtype, \n",
    "                                crs=upsampled_raster.rio.crs, \n",
    "                                transform=upsampled_raster.rio.transform()) as dataset:\n",
    "                    dataset.write(upsampled_raster.values, 1)\n",
    "\n",
    "                # Load the shapefile for zonal statistics\n",
    "                gdf = gpd.read_file(extent_filename)\n",
    "                gdf = gdf[['gid', 'geometry', 'xcoord', 'ycoord']]\n",
    "\n",
    "                # Calculate zonal statistics on the upsampled raster\n",
    "                stats = zonal_stats(gdf, memfile, stats='mean', geojson_out=True)\n",
    "                stats_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "                # Add year and month fields\n",
    "                stats_gdf['year'] = year\n",
    "                stats_gdf['month'] = month\n",
    "                stats_gdf.rename(columns={'mean': param_climate_index}, inplace=True)\n",
    "\n",
    "                # Ensure stats_gdf has valid geometry and data\n",
    "                stats_gdf = stats_gdf[stats_gdf.geometry.notnull() & stats_gdf[param_climate_index].notnull()]\n",
    "            del upsampled_raster  # Clean up if no longer needed\n",
    "\n",
    "        # Plot the zonal statistics if there is data\n",
    "        if not stats_gdf.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            stats_gdf.plot(column=param_climate_index, ax=ax, legend=True, cmap='viridis', edgecolor='none')\n",
    "            ax.set_title(f'{param_climate_index} Statistics by Region - {year}-{month}')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "            #plt.show()\n",
    "            plot_figures.append(fig)  # Append figure to list\n",
    "            plt.close(fig)\n",
    "\n",
    "        else:\n",
    "            print(f\"No valid zonal statistics data to plot for {param_climate_index} at time index {i}\")\n",
    "\n",
    "        # Append to list\n",
    "        all_stats.append(stats_gdf)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    final_gdf = pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "    # Save final DataFrame to CSV in the designated folder\n",
    "\n",
    "    file_name = f\"{param_climate_index}_{temporal_attribution}__centroid_process.csv\"\n",
    "    output_file_path = generated_index_table_folder / file_name\n",
    "\n",
    "\n",
    "    final_gdf.to_csv(output_file_path, index=False)\n",
    "    print(f\"Final DataFrame saved to: {output_file_path}\")\n",
    "\n",
    "    generate_layout_and_save(param_time_index_list, plot_figures, map_folder, param_climate_index)\n",
    "\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time index: 0\n",
      "Year: 2015 Month: 06\n",
      "CRS is not set. Setting CRS to EPSG:4326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenz/Documents/Climate Data/climate_extremes/utils/etccdi_to_pg__pointquery.py:115: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid'] = gdf.geometry.centroid\n",
      "/Users/gbenz/miniforge3/envs/viewser/lib/python3.11/site-packages/rasterstats/io.py:335: NodataWarning: Setting nodata to -999; specify nodata explicitly\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time index: 1\n",
      "Year: 2016 Month: 06\n",
      "CRS is not set. Setting CRS to EPSG:4326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenz/Documents/Climate Data/climate_extremes/utils/etccdi_to_pg__pointquery.py:115: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['centroid'] = gdf.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame saved to: /Users/gbenz/Documents/Climate Data/climate_extremes/data/generated/index_table_output/idETCCDI_yearly_201506_201606__centroid_process.csv\n",
      "idETCCDI_yearly_201506_201606__centroid_process.csv\n",
      "All graphics saved to /Users/gbenz/Documents/Climate Data/climate_extremes/docs/Graphics/Standard_review/idETCCDI_layout.pdf\n"
     ]
    }
   ],
   "source": [
    "if method == 'raster_query':\n",
    "    translated_filename = generate_etccdi_temporal_tables__centroid(index_list, etccdi, etccdi_index, report_temporal_dimensions, save_tif)\n",
    "\n",
    "elif method == 'resample':\n",
    "    translated_filename = generate_etccdi_temporal_tables(index_list, etccdi, etccdi_index, report_temporal_dimensions, save_tif)\n",
    "\n",
    "else: \n",
    "    print('you have entered a bad prompt for the method parameter. Please restart.... ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``translated_filename`` retrieves the summary table saved to:\n",
    "\n",
    "\n",
    "``reference_filtered_time`` retrieves the primary reference table saved to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There was an error in report_null_etccdi_values:\n",
    "proceeding code line concentrates on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idETCCDI\n"
     ]
    }
   ],
   "source": [
    "index = str(translated_filename).split('_')[0]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values in 'validate_etccdi' DataFrame: 0\n",
      "Total number of null values in 'validate_etccdi' DataFrame: 0\n",
      "therefore, there is no null summary dataframe to report here!\n"
     ]
    }
   ],
   "source": [
    "report_null_etccdi_values(translated_filename, reference_filtered_time, temporal_aggregation_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you can migrate to the ingestion script\n",
    "\n",
    "We want to intentionally keep these things seperate (lock / key) so unwanted things are not automatically ingested\n",
    "\n",
    "- **Clarify migration to the ingestion script**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Now you are ready to run the ingestion code located in .... \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
