{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we have downloaded a net CDF from the Copernicus Data Store: https://cds.climate.copernicus.eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/gbenz/Downloads/tx10pETCCDI_mon_HadGEM3-GC31-LL_historical_r1i1p1f3_b1981-2010_v20190624_185001-201412_v2-0.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from rasterstats import zonal_stats\n",
    "import numpy as np\n",
    "\n",
    "from utils.unzip import unzip_etccdi_package\n",
    "from utils.correct_longitude import transform_longitudinal_values\n",
    "from utils.give_metadata import give_metadata\n",
    "from utils.etccdi_to_pg import generate_etccdi_temporal_tables\n",
    "from utils.temporal_index import find_etccdi_timeindex\n",
    "from utils.define_request import generate_and_validate_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access with Copernicus Data Store API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective by Oct. 30 is to have this process begin by retreiving (predefined and approved) ETCCDI data parameters from an API\n",
    "\n",
    "- Works on Mon 28 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proceeding code provides a correct output but requires a unique user API key. This cannot be simplified much further.\n",
    "\n",
    "The desirable output is to exclusively change:\n",
    "- 'variable' \n",
    "- product_type\n",
    "- period\n",
    "#### ------------------\n",
    "- start_year = '1995'\n",
    "- start_month = '01'\n",
    "- end_year = '2000'\n",
    "- end_month = '12'\n",
    "\n",
    "Then if you select 'cold days' a decision tree will be printed with the optional parameters that could be selected for product type and period\n",
    "Other parameters will be kept standard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df_y = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg__y.csv', index_col=None)\n",
    "reference_df_y = reference_df_y.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Convert 'year' to string\n",
    "reference_df_y['year'] = reference_df_y['year'].astype(str)\n",
    "\n",
    "print(reference_df_y.dtypes)\n",
    "\n",
    "reference_df_m = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg__m.csv', index_col=None)\n",
    "reference_df_m = reference_df_m.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Convert 'year' to string\n",
    "reference_df_m['year'] = reference_df_m['year'].astype(str)\n",
    "\n",
    "# Convert 'month' to an integer first (removes decimals) and then to string\n",
    "reference_df_m['month'] = reference_df_m['month'].astype(int).astype(str)\n",
    "\n",
    "print(reference_df_m.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          month_id  priogrid_gid  year month\n",
      "0                1         62356  1980     1\n",
      "1                1         79599  1980     1\n",
      "2                1         79600  1980     1\n",
      "3                1         79601  1980     1\n",
      "4                1         80317  1980     1\n",
      "...            ...           ...   ...   ...\n",
      "11169715       852        190496  2050    12\n",
      "11169716       852        190507  2050    12\n",
      "11169717       852        190508  2050    12\n",
      "11169718       852        190510  2050    12\n",
      "11169719       852        190511  2050    12\n",
      "\n",
      "[11169720 rows x 4 columns]\n",
      "month_id         int64\n",
      "priogrid_gid     int64\n",
      "year            object\n",
      "month           object\n",
      "dtype: object\n",
      "        priogrid_gid  year\n",
      "0              62356  1980\n",
      "1              79599  1980\n",
      "2              79600  1980\n",
      "3              79601  1980\n",
      "4              80317  1980\n",
      "...              ...   ...\n",
      "930805        190496  2050\n",
      "930806        190507  2050\n",
      "930807        190508  2050\n",
      "930808        190510  2050\n",
      "930809        190511  2050\n",
      "\n",
      "[930810 rows x 2 columns]\n",
      "priogrid_gid     int64\n",
      "year            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(reference_df_m)\n",
    "print(reference_df_m.dtypes)\n",
    "reference_df_m.to_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg___m.csv')\n",
    "\n",
    "print(reference_df_y)\n",
    "print(reference_df_y.dtypes)\n",
    "\n",
    "reference_df_y.to_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg___y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0      int64\n",
      "priogrid_gid    int64\n",
      "year            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "ref = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg___y.csv')\n",
    "\n",
    "print(ref.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request is valid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'variable': ['consecutive_dry_days'],\n",
       " 'product_type': ['base_independent'],\n",
       " 'model': ['hadgem3_gc31_ll'],\n",
       " 'ensemble_member': ['r1i1p1f3'],\n",
       " 'experiment': ['historical'],\n",
       " 'temporal_aggregation': ['yearly'],\n",
       " 'period': ['1850_2014'],\n",
       " 'version': ['2_0'],\n",
       " 'data_format': 'netcdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priogrid_gid     int64\n",
      "year            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Now, calling the function will generate and validate the request\n",
    "request = generate_and_validate_request(\n",
    "    variable=\"consecutive_dry_days\",\n",
    "    product_type=\"base_independent\",\n",
    "    experiment=\"historical\",\n",
    "    temporal_aggregation=\"yearly\"\n",
    ")\n",
    "\n",
    "display(request)\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Load a clean PG dataframe at a consistent temporal resolution\n",
    "# to the request built\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "temporal_aggregation_value = request['temporal_aggregation'][0]\n",
    "\n",
    "if temporal_aggregation_value == 'yearly':\n",
    "    reference_df = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg__y.csv')\n",
    "\n",
    "    reference_df= reference_df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "    # Convert 'year' to string\n",
    "    reference_df['year'] = reference_df['year'].astype(str)\n",
    "\n",
    "    print(reference_df.dtypes)\n",
    "\n",
    "else:\n",
    "    reference_df = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/data/processed/pg__m.csv')\n",
    "\n",
    "    reference_df = reference_df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "    # Convert 'year' to string\n",
    "    reference_df['year'] = reference_df['year'].astype(str)\n",
    "\n",
    "    # Convert 'month' to an integer first (removes decimals) and then to string\n",
    "    reference_df['month'] = reference_df['month'].astype(int).astype(str)\n",
    "\n",
    "    print(reference_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:11:12,180 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-12 09:11:12,181 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-12 09:11:12,182 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-12 09:11:12,183 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-12 09:11:12,480 INFO Request ID is 2d6de506-a660-4246-825c-ef5b60341df8\n",
      "2024-11-12 09:11:12,585 INFO status has been updated to accepted\n",
      "2024-11-12 09:11:23,672 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb9ef8ef7e346cca5b4fd3dc0ea67db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "432716f0a4b4f4f0049e5f147da23357.zip:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'consecutive_dry_days_yearly_1850_2014.zip'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cdsapi\n",
    "\n",
    "dataset = \"sis-extreme-indices-cmip6\"\n",
    "\n",
    "# Extract the desired elements from the request dictionary\n",
    "variable = request[\"variable\"][0]\n",
    "temporal_aggregation = request[\"temporal_aggregation\"][0]\n",
    "period = request[\"period\"][0]\n",
    "\n",
    "# Concatenate them with an underscore or any other separator you prefer\n",
    "zip_file_name = f\"{variable}_{temporal_aggregation}_{period}.zip\"\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request, target=zip_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cddETCCDI\n",
      "Extracted file names: cddETCCDI_yr_HadGEM3-GC31-LL_historical_r1i1p1f3_no-base_v20190624_1850-2014_v2-0.nc\n"
     ]
    }
   ],
   "source": [
    "netcdf_file, etccdi_index = unzip_etccdi_package(zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Metadata from the selected ETTCDI netCDF file:\n",
    "\n",
    "Move the pg shapefile to the github repo so this can be accessed without references to local paths\n",
    "\n",
    "Accomplishes:\n",
    "- checks to ensure the correct netcdf file is being processed\n",
    "- provides spatial and temporal metadata\n",
    "\n",
    "From preprocessing, we know that the ETCCDI climate data is not packaged in a desirable format, that is, the original longitudinal range is: 0.9375 to 359.0625\n",
    "- Adjust the Longitude range \n",
    "- save an 'adjusted netcdf' file.\n",
    "\n",
    "\n",
    "28-10 -- What would perhaps be most desirable is to first transform, then, report metadata with two seperate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable 'cddETCCDI' was found in the file path and the world continues to spin.\n",
      "Original Latitude range: -89.375 to 89.375\n",
      "Original Longitude range: 0.9375 to 359.0625\n",
      "Adjusted Longitude range: -179.0625 to 179.0625\n",
      "Adjusted dataset saved to: /Users/gbenz/Downloads/adjusted_cddETCCDI_yr_HadGEM3-GC31-LL_historical_r1i1p1f3_no-base_v20190624_1850-2014_v2-0.nc.nc\n"
     ]
    }
   ],
   "source": [
    "etccdi = transform_longitudinal_values(etccdi_index, netcdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude range: -89.375 to 89.375\n",
      "Longitude range: -179.0625 to 179.0625\n",
      "Latitude resolution: 1.25\n",
      "Longitude resolution: 1.875\n",
      "Global Metadata:\n",
      "CDI: Climate Data Interface version 1.8.0 (http://mpimet.mpg.de/cdi)\n",
      "history: Tue Nov 24 08:58:40 2020: cdo mergetime tasmax_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_18500101-19491230.nc tasmax_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_19500101-20141230.nc ./merged/tasmax_day_HadGEM3-GC31-LL_historical_r1i1p1f3_gn_18500101-20141230.nc\n",
      "2019-06-19T11:16:23Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.;\n",
      "2019-06-19T11:07:16Z MIP Convert v1.1.0, Python v2.7.12, Iris v1.13.0, Numpy v1.13.3, netcdftime v1.4.1.\n",
      "source: HadGEM3-GC31-LL (2016): \n",
      "aerosol: UKCA-GLOMAP-mode\n",
      "atmos: MetUM-HadGEM3-GA7.1 (N96; 192 x 144 longitude/latitude; 85 levels; top level 85 km)\n",
      "atmosChem: none\n",
      "land: JULES-HadGEM3-GL7.1\n",
      "landIce: none\n",
      "ocean: NEMO-HadGEM3-GO6.0 (eORCA1 tripolar primarily 1 deg with meridional refinement down to 1/3 degree in the tropics; 360 x 330 longitude/latitude; 75 levels; top grid cell 0-1 m)\n",
      "ocnBgchem: none\n",
      "seaIce: CICE-HadGEM3-GSI8 (eORCA1 tripolar primarily 1 deg; 360 x 330 longitude/latitude)\n",
      "institution: Met Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n",
      "Conventions: CF-1.7 CMIP-6.2\n",
      "activity_id: CMIP\n",
      "branch_method: standard\n",
      "branch_time_in_child: 0.0\n",
      "branch_time_in_parent: 0.0\n",
      "input_creation_date: 2019-06-19T14:20:08Z\n",
      "cv_version: 6.2.20.1\n",
      "data_specs_version: 01.00.29\n",
      "experiment: all-forcing simulation of the recent past\n",
      "experiment_id: historical\n",
      "external_variables: areacella\n",
      "forcing_index: 3\n",
      "input_frequency: day\n",
      "further_info_url: https://furtherinfo.es-doc.org/CMIP6.MOHC.HadGEM3-GC31-LL.historical.none.r1i1p1f3\n",
      "grid: Native N96 grid; 192 x 144 longitude/latitude\n",
      "grid_label: gn\n",
      "initialization_index: 1\n",
      "institution_id: MOHC\n",
      "mip_era: CMIP6\n",
      "mo_runid: u-bg466\n",
      "nominal_resolution: 250 km\n",
      "parent_activity_id: CMIP\n",
      "parent_experiment_id: piControl\n",
      "parent_mip_era: CMIP6\n",
      "parent_source_id: HadGEM3-GC31-LL\n",
      "parent_time_units: days since 1850-01-01-00-00-00\n",
      "parent_variant_label: r1i1p1f1\n",
      "physics_index: 1\n",
      "product: model-output\n",
      "realization_index: 1\n",
      "realm: atmos\n",
      "source_id: HadGEM3-GC31-LL\n",
      "source_type: AOGCM AER\n",
      "sub_experiment: none\n",
      "sub_experiment_id: none\n",
      "table_id: day\n",
      "table_info: Creation Date:(13 December 2018) MD5:2b12b5db6db112aa8b8b0d6c1645b121\n",
      "input_title: HadGEM3-GC31-LL output prepared for CMIP6\n",
      "variable_id: tasmax\n",
      "variant_label: r1i1p1f3\n",
      "license: CMIP6 model data produced by the Met Office Hadley Centre is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file) and at https://ukesm.ac.uk/cmip6. The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.\n",
      "cmor_version: 3.4.0\n",
      "input_tracking_id: hdl:21.14100/ce656162-9898-47ca-be4f-ef0dec151bfc\n",
      "CDO: Climate Data Operators version 1.8.0 (http://mpimet.mpg.de/cdo)\n",
      "ETCCDI_institution: Center for International Climate and Environmental Research - Oslo, Norway\n",
      "ETCCDI_institution_id: CICERO\n",
      "ETCCDI_software: climdex.pcic\n",
      "ETCCDI_software_version: 1.1.9.1\n",
      "frequency: yr\n",
      "creation_date: 2020-11-24T20:15:31Z\n",
      "title: ETCCDI indices computed on HadGEM3-GC31-LL output prepared for CMIP6\n",
      "Unique Years: [1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]\n",
      "Unique Months: [6]\n"
     ]
    }
   ],
   "source": [
    "give_metadata(etccdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0      int64\n",
      "priogrid_gid    int64\n",
      "year            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(reference_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for 01 of the year 1990 but located data for the first available month.\n",
      "Validation: Found data for Year: 1990, Month: 06 at index 140.\n",
      "No data found for 12 of the year 1993 but located data for the first available month.\n",
      "Validation: Found data for Year: 1993, Month: 06 at index 143.\n",
      "The start index is: 140, referencing Month: 06 and Year: 1990\n",
      "The end index is: 143, referencing Month: 06 and Year: 1993\n",
      "        priogrid_gid  year\n",
      "131100         62356  1990\n",
      "131101         79599  1990\n",
      "131102         79600  1990\n",
      "131103         79601  1990\n",
      "131104         80317  1990\n",
      "...              ...   ...\n",
      "183535        190496  1993\n",
      "183536        190507  1993\n",
      "183537        190508  1993\n",
      "183538        190510  1993\n",
      "183539        190511  1993\n",
      "\n",
      "[52440 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Define Start Year & Month\n",
    "#-----------------------------------------------------------\n",
    "start_year = '1990'\n",
    "start_month = '01'\n",
    "#-----------------------------------------------------------\n",
    "# Define End Year & Month\n",
    "end_year = '1993'\n",
    "end_month = '12'\n",
    "#-----------------------------------------------------------\n",
    "#-----------------------------------------------------------\n",
    "# Establish Start and End index values:\n",
    "start_index_val, loc_start_month, loc_start_year =  find_etccdi_timeindex(start_year, start_month, etccdi)\n",
    "end_index_val, loc_end_month, loc_end_year = find_etccdi_timeindex(end_year, end_month, etccdi)\n",
    "#-----------------------------------------------------------\n",
    "print(f'The start index is: {start_index_val}, referencing Month: {loc_start_month} and Year: {loc_start_year}')\n",
    "print(f'The end index is: {end_index_val}, referencing Month: {loc_end_month} and Year: {loc_end_year}')\n",
    "#-----------------------------------------------------------\n",
    "index_list = list(range(start_index_val, end_index_val + 1))\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Filter the PG reference file to the temporal parameters now established:\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# For annual:\n",
    "# the etccdi dataframe will contain a month field but this is irrelevant because the temporal resolution is 1-year\n",
    "\n",
    "# For monthly:\n",
    "# why don't you filter for a monthly attribute?: Because all months will be included when subsetting by year.\n",
    "reference_filtered_time = reference_df.loc[(reference_df['year'] >= loc_start_year) & (reference_df['year'] <= loc_end_year)]\n",
    "#-----------------------------------------------------------\n",
    "print(reference_filtered_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the first n elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "25\n",
      "[140, 141]\n"
     ]
    }
   ],
   "source": [
    "sub_index = index_list[:2]\n",
    "\n",
    "time_length_subset = len(sub_index)\n",
    "time_length = len(index_list)\n",
    "\n",
    "print(time_length_subset)\n",
    "print(time_length)\n",
    "\n",
    "print(sub_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puts it all together\n",
    "\n",
    "Parameters:\n",
    "1. references the sub_index which supplies the list (or sublist) of indexes to iterate over. Index specifically references time. This can be confusing because the ETCCDI variables are themselves climate indices.\n",
    "2. Creates a single geotiff from the current time selection. We do this because the NetCDF itself is not a format that can be incorporated into rigorous analysis so as we iterate through the time series we convert the working item to a geotiff which is a format that can be operated on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "\n",
    "1. NetCDF file\n",
    "2. (TEMPORAL) sub_index or full index (specify index to loop over)\n",
    "3. etccdi index ex(tx10pETCCDI)\n",
    "\n",
    "\n",
    "ADF -- Decision to just save to ONE WORKING raster that will continously be rewritten \n",
    "rationale: The purpose of having unique tifs is to visualize holes in the data. However, this is not worth the space. If holes appear in the tabular dataset, a new geotiff corresponding to that month / year can quickly be produced!\n",
    "\n",
    "ADF -- Rationalize why this is best:\n",
    "    # Resample the raster data to the new resolution\n",
    "    resampled_raster = raster_data.rio.reproject(\n",
    "        raster_data.rio.crs,\n",
    "        shape=(\n",
    "            int(raster_data.shape[1] * 10),  # Increase number of rows by a factor of 10\n",
    "            int(raster_data.shape[2] * 10)   # Increase number of columns by a factor of 10\n",
    "        ),\n",
    "        resampling=Resampling.bilinear  # Use the correct resampling method\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:\n",
    "\n",
    "- time_index_list,\n",
    "- netcdf, climate_index, \n",
    "- shapefile_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_etccdi_temporal_tables(param_time_index_list, param_netcdf, param_climate_index, param_shapefile_path):\n",
    "    all_stats = []\n",
    "\n",
    "    # Retrieve the first and last time indices for file naming\n",
    "    first_time_index = param_time_index_list[0]\n",
    "    last_time_index = param_time_index_list[-1]\n",
    "\n",
    "    for i in param_time_index_list:\n",
    "        print(f\"Processing time index: {i}\")\n",
    "        \n",
    "        # Select the data for the specified climate index\n",
    "        data = param_netcdf[param_climate_index]\n",
    "        \n",
    "        # Check the data type and process accordingly\n",
    "        data_type = data.dtype\n",
    "        if data_type == 'timedelta64[ns]':\n",
    "            data_days = data / np.timedelta64(1, 'D')  # Convert to days if it's timedelta\n",
    "            raster_data = data_days.isel(time=i)\n",
    "        elif data_type == 'float32':\n",
    "            raster_data = data.isel(time=i)  # Use as-is if it's already float32\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type '{data_type}' for variable '{param_climate_index}'. Expected 'timedelta64[ns]' or 'float32'.\")\n",
    "        \n",
    "        # Plotting and other processing steps\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        raster_data.plot(cmap='viridis')\n",
    "        plt.title(f'{param_climate_index} at Time Index {i}')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "        # Convert spatial dimensions\n",
    "        raster_data = raster_data.rename({'lon': 'x', 'lat': 'y'})\n",
    "        raster_data = raster_data.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "\n",
    "        # Get the date and time information\n",
    "        date_time = str(param_netcdf['time'].isel(time=i).values.item())\n",
    "        year = date_time.split('-')[0]\n",
    "        month = date_time.split('-')[1]\n",
    "        print(\"Year:\", date_time)\n",
    "\n",
    "        # Set CRS if not already defined\n",
    "        if not raster_data.rio.crs:\n",
    "            print(\"CRS is not set. Setting CRS to EPSG:4326\")\n",
    "            raster_data = raster_data.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Check bounds and transform\n",
    "        print(\"Bounds before saving:\", raster_data.rio.bounds())\n",
    "        print(\"Transform before saving:\", raster_data.rio.transform())\n",
    "\n",
    "        # Convert to float for consistent data type in raster operations\n",
    "        raster_data = raster_data.astype('float32')\n",
    "\n",
    "        # Check for NaN values and mask if needed (Do not overwrite NaNs to 0 here)\n",
    "        print(\"NaN values check before masking:\", raster_data.isnull().sum().item())\n",
    "        raster_data = raster_data.where(~np.isnan(raster_data), other=np.nan)  # Keep NaNs for now\n",
    "        \n",
    "        # Save the raster to GeoTIFF\n",
    "        raster_file_path = 'working_etccdi_file.tif'\n",
    "        raster_data.rio.to_raster(raster_file_path)\n",
    "        print(f\"GeoTIFF saved at: {raster_file_path}\")\n",
    "\n",
    "        # Resample Raster (with NaNs preserved)\n",
    "        raster_data = rioxarray.open_rasterio(raster_file_path)\n",
    "\n",
    "        # Calculate current and new resolutions\n",
    "        current_resolution_x = abs(raster_data.x[1] - raster_data.x[0])\n",
    "        current_resolution_y = abs(raster_data.y[1] - raster_data.y[0])\n",
    "        new_resolution_x = current_resolution_x / 10\n",
    "        new_resolution_y = current_resolution_y / 10\n",
    "\n",
    "        # Resample without introducing NoData values\n",
    "        print(\"Resampling raster...\")\n",
    "        resampled_raster = raster_data.rio.reproject(\n",
    "            raster_data.rio.crs,\n",
    "            shape=(\n",
    "                int(raster_data.shape[1] * 10),  \n",
    "                int(raster_data.shape[2] * 10)  \n",
    "            ),\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "\n",
    "        # Check bounds and transform after resampling\n",
    "        print(\"Bounds after resampling:\", resampled_raster.rio.bounds())\n",
    "        print(\"Transform after resampling:\", resampled_raster.rio.transform())\n",
    "\n",
    "        # Save resampled raster\n",
    "        resampled_raster_path = 'working_etccdi_file_resampled.tif'\n",
    "        resampled_raster.rio.to_raster(resampled_raster_path)\n",
    "        print(f\"Resampled GeoTIFF saved at: {resampled_raster_path}\")\n",
    "\n",
    "        # Calculate zonal statistics\n",
    "        gdf = gpd.read_file(param_shapefile_path)\n",
    "        gdf = gdf[['gid', 'geometry', 'xcoord', 'ycoord']]\n",
    "        stats = zonal_stats(gdf, resampled_raster_path, stats='mean', geojson_out=True)\n",
    "        stats_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "        # Add Year and Month fields to stats_gdf\n",
    "        stats_gdf['year'] = year\n",
    "        stats_gdf['month'] = month\n",
    "        stats_gdf.rename(columns={'mean': param_climate_index}, inplace=True)\n",
    "\n",
    "        # Plot the zonal statistics\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        stats_gdf.plot(column=param_climate_index, ax=ax, legend=True, cmap='viridis', edgecolor='none')\n",
    "        ax.set_title(f'{param_climate_index} Statistics by Region - {year}-{month}')\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "        # Append the stats_gdf to the all_stats list\n",
    "        all_stats.append(stats_gdf)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_gdf = pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "    # Construct the output filename\n",
    "    first_date_time = str(param_netcdf['time'].isel(time=first_time_index).values.item())\n",
    "    last_date_time = str(param_netcdf['time'].isel(time=last_time_index).values.item())\n",
    "    first_year, first_month = first_date_time.split('-')[0], first_date_time.split('-')[1]\n",
    "    last_year, last_month = last_date_time.split('-')[0], last_date_time.split('-')[1]\n",
    "    \n",
    "    # Save the final DataFrame to a CSV file\n",
    "    folder = 'etccdi_out_files'\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    output_file_path = os.path.join(folder, f\"{param_climate_index}_{first_year}_{first_month}__{last_year}_{last_month}.csv\")\n",
    "    \n",
    "    final_gdf.to_csv(output_file_path, index=False)\n",
    "    print(f\"Final DataFrame saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def generate_etccdi_temporal_tables(param_time_index_list, param_netcdf, param_climate_index, param_shapefile_path):\n",
    "    all_stats = []\n",
    "\n",
    "    # Retrieve the first and last time indices for file naming\n",
    "    first_time_index = param_time_index_list[0]\n",
    "    last_time_index = param_time_index_list[-1]\n",
    "\n",
    "    for i in param_time_index_list:\n",
    "        print(f\"Processing time index: {i}\")\n",
    "        \n",
    "        # Select the data for the specified climate index\n",
    "        data = param_netcdf[param_climate_index]\n",
    "        \n",
    "        # Check the data type and process accordingly\n",
    "        data_type = data.dtype\n",
    "        if data_type == 'timedelta64[ns]':\n",
    "            data_days = data / np.timedelta64(1, 'D')  # Convert to days if it's timedelta\n",
    "            raster_data = data_days.isel(time=i)\n",
    "        elif data_type == 'float32':\n",
    "            raster_data = data.isel(time=i)  # Use as-is if it's already float32\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type '{data_type}' for variable '{param_climate_index}'. Expected 'timedelta64[ns]' or 'float32'.\")\n",
    "        \n",
    "        # Convert spatial dimensions\n",
    "        raster_data = raster_data.rename({'lon': 'x', 'lat': 'y'})\n",
    "        raster_data = raster_data.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "\n",
    "        # Get the date and time information\n",
    "        date_time = str(param_netcdf['time'].isel(time=i).values.item())\n",
    "        year, month = date_time.split('-')[:2]\n",
    "        print(\"Year:\", year, \"Month:\", month)\n",
    "\n",
    "        # Set CRS if not already defined\n",
    "        if not raster_data.rio.crs:\n",
    "            print(\"CRS is not set. Setting CRS to EPSG:4326\")\n",
    "            raster_data = raster_data.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Resample with interpolation to fill NaN values\n",
    "        def resample_and_interpolate(raster_data, factor=3):\n",
    "            # Perform resampling using bilinear interpolation (to fill NaN values)\n",
    "            upsampled_raster = raster_data.rio.reproject(\n",
    "                raster_data.rio.crs,\n",
    "                shape=(\n",
    "                    int(raster_data.sizes['y'] * factor),\n",
    "                    int(raster_data.sizes['x'] * factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            \n",
    "            # Interpolate to fill NaN values\n",
    "            upsampled_raster = upsampled_raster.interpolate_na(dim='x', method='linear').interpolate_na(dim='y', method='linear')\n",
    "            \n",
    "            return upsampled_raster\n",
    "\n",
    "        # Resample with interpolation\n",
    "        upsampled_raster = resample_and_interpolate(raster_data, factor=3)\n",
    "\n",
    "        # Plot the resampled raster with NaNs filled\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        upsampled_raster.plot(cmap='viridis')\n",
    "        plt.title(f'Upsampled and Interpolated {param_climate_index} at Time Index {i}')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "        # Save the resampled raster to GeoTIFF\n",
    "        upsampled_raster_path = 'working_etccdi_file_upsampled_interpolated.tif'\n",
    "        upsampled_raster.rio.to_raster(upsampled_raster_path)\n",
    "        print(f\"Upsampled GeoTIFF saved at: {upsampled_raster_path}\")\n",
    "\n",
    "        # Create a null mask for the original raster\n",
    "        null_mask = raster_data.isnull()\n",
    "\n",
    "        # Convert the null mask into polygons (vectorize the mask)\n",
    "        null_mask_poly = null_mask.rio.to_raster('null_mask.tif')\n",
    "        null_mask_gdf = gpd.read_file('null_mask.tif')\n",
    "\n",
    "        # Load the shapefile for zonal statistics\n",
    "        gdf = gpd.read_file(param_shapefile_path)\n",
    "        gdf = gdf[['gid', 'geometry', 'xcoord', 'ycoord']]\n",
    "\n",
    "        # Calculate zonal statistics on the upsampled raster\n",
    "        stats = zonal_stats(gdf, upsampled_raster_path, stats='mean', geojson_out=True)\n",
    "        stats_gdf = gpd.GeoDataFrame.from_features(stats)\n",
    "\n",
    "        # Assign NaN values to zonal statistics if the zone falls within the null mask\n",
    "        for index, row in stats_gdf.iterrows():\n",
    "            # Check if the centroid of the zone is inside the null mask\n",
    "            zone_centroid = row.geometry.centroid\n",
    "            if null_mask_gdf.geometry.contains(zone_centroid).any():\n",
    "                stats_gdf.at[index, param_climate_index] = np.nan\n",
    "\n",
    "        # Add year and month fields\n",
    "        stats_gdf['year'] = year\n",
    "        stats_gdf['month'] = month\n",
    "        stats_gdf.rename(columns={'mean': param_climate_index}, inplace=True)\n",
    "\n",
    "        # Ensure stats_gdf has valid geometry and data\n",
    "        stats_gdf = stats_gdf[stats_gdf.geometry.notnull() & stats_gdf[param_climate_index].notnull()]\n",
    "\n",
    "        # Plot the zonal statistics if there is data\n",
    "        if not stats_gdf.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            stats_gdf.plot(column=param_climate_index, ax=ax, legend=True, cmap='viridis', edgecolor='none')\n",
    "            ax.set_title(f'{param_climate_index} Statistics by Region - {year}-{month}')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No valid zonal statistics data to plot for {param_climate_index} at time index {i}\")\n",
    "\n",
    "        # Append to list\n",
    "        all_stats.append(stats_gdf)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    final_gdf = pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "    # Save final DataFrame to CSV\n",
    "    folder = 'etccdi_out_files'\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    output_file_path = os.path.join(folder, f\"{param_climate_index}_{first_time_index}_{last_time_index}.csv\")\n",
    "    \n",
    "    final_gdf.to_csv(output_file_path, index=False)\n",
    "    print(f\"Final DataFrame saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADF: Treating NaN values in the Raster \n",
    "\n",
    "### Options\n",
    "Mask Out NaNs in the Original Raster Before Resampling:\n",
    "- Create a mask to identify NaNs in the original raster.\n",
    "- Use an interpolation method that only processes non-NaN cells, allowing it to \"skip\" over NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time index: 140\n",
      "Year: 1990 Month: 06\n",
      "CRS is not set. Setting CRS to EPSG:4326\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index 'y' must be monotonically increasing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbenz/Documents/Climate Data/climate_extremes/oct_processing.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_etccdi_temporal_tables(index_list, etccdi, etccdi_index, \u001b[39m'\u001b[39;49m\u001b[39m/Users/gbenz/Downloads/pg_extent/pgm_viewser_extent.shp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/gbenz/Documents/Climate Data/climate_extremes/oct_processing.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m upsampled_raster\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Resample with interpolation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m upsampled_raster \u001b[39m=\u001b[39m resample_and_interpolate(raster_data, factor\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Plot the resampled raster with NaNs filled\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m6\u001b[39m))\n",
      "\u001b[1;32m/Users/gbenz/Documents/Climate Data/climate_extremes/oct_processing.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m upsampled_raster \u001b[39m=\u001b[39m raster_data\u001b[39m.\u001b[39mrio\u001b[39m.\u001b[39mreproject(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     raster_data\u001b[39m.\u001b[39mrio\u001b[39m.\u001b[39mcrs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     shape\u001b[39m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     resampling\u001b[39m=\u001b[39mResampling\u001b[39m.\u001b[39mbilinear\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Interpolate to fill NaN values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m upsampled_raster \u001b[39m=\u001b[39m upsampled_raster\u001b[39m.\u001b[39;49minterpolate_na(dim\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49minterpolate_na(dim\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m'\u001b[39;49m, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X23sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m upsampled_raster\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/xarray/core/dataarray.py:3593\u001b[0m, in \u001b[0;36mDataArray.interpolate_na\u001b[0;34m(self, dim, method, limit, use_coordinate, max_gap, keep_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   3498\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fill in NaNs by interpolating according to different methods.\u001b[39;00m\n\u001b[1;32m   3499\u001b[0m \n\u001b[1;32m   3500\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3589\u001b[0m \u001b[39m  * x        (x) int64 40B 0 1 2 3 4\u001b[39;00m\n\u001b[1;32m   3590\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3591\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxarray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmissing\u001b[39;00m \u001b[39mimport\u001b[39;00m interp_na\n\u001b[0;32m-> 3593\u001b[0m \u001b[39mreturn\u001b[39;00m interp_na(\n\u001b[1;32m   3594\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3595\u001b[0m     dim\u001b[39m=\u001b[39;49mdim,\n\u001b[1;32m   3596\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m   3597\u001b[0m     limit\u001b[39m=\u001b[39;49mlimit,\n\u001b[1;32m   3598\u001b[0m     use_coordinate\u001b[39m=\u001b[39;49muse_coordinate,\n\u001b[1;32m   3599\u001b[0m     max_gap\u001b[39m=\u001b[39;49mmax_gap,\n\u001b[1;32m   3600\u001b[0m     keep_attrs\u001b[39m=\u001b[39;49mkeep_attrs,\n\u001b[1;32m   3601\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3602\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/xarray/core/missing.py:355\u001b[0m, in \u001b[0;36minterp_na\u001b[0;34m(self, dim, use_coordinate, method, limit, max_gap, keep_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected integer or floating point max_gap since use_coordinate=False. Received \u001b[39m\u001b[39m{\u001b[39;00mmax_type\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m             )\n\u001b[1;32m    354\u001b[0m \u001b[39m# method\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m index \u001b[39m=\u001b[39m get_clean_interp_index(\u001b[39mself\u001b[39;49m, dim, use_coordinate\u001b[39m=\u001b[39;49muse_coordinate)\n\u001b[1;32m    356\u001b[0m interp_class, kwargs \u001b[39m=\u001b[39m _get_interpolator(method, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    357\u001b[0m interpolator \u001b[39m=\u001b[39m partial(func_interpolate_na, interp_class, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/viewser/lib/python3.11/site-packages/xarray/core/missing.py:282\u001b[0m, in \u001b[0;36mget_clean_interp_index\u001b[0;34m(arr, dim, use_coordinate, strict)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m strict:\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m index\u001b[39m.\u001b[39mis_monotonic_increasing:\n\u001b[0;32m--> 282\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mname\u001b[39m!r}\u001b[39;00m\u001b[39m must be monotonically increasing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m index\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m    285\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndex \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m.\u001b[39mname\u001b[39m!r}\u001b[39;00m\u001b[39m has duplicate values\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Index 'y' must be monotonically increasing"
     ]
    }
   ],
   "source": [
    "generate_etccdi_temporal_tables(index_list, etccdi, etccdi_index, '/Users/gbenz/Downloads/pg_extent/pgm_viewser_extent.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review:\n",
    "\n",
    "#### Validate completeness of the output index at PG resolution:\n",
    "\n",
    "Temporally: check that all total number of time periods match\n",
    "- Spatially: Full extent of PG (for each temporal unit!)\n",
    "\n",
    "How to do this:\n",
    "\n",
    "1. Load the 'compiled' etccdi index .csv\n",
    "2. check for null values\n",
    "\n",
    "Total length should be: X\n",
    "\n",
    "check length of temporal units should be: X (dependent on input parameters)\n",
    "check length of spatial units should be: X\n",
    "\n",
    "3. Plot the data\n",
    "\n",
    "### What we need:\n",
    "\n",
    "A complete 'clean' dataframe to reference from VIEWSER! 11.11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_monthly_average_tx10pETCCDI(dataframe):\n",
    "    # Group by 'date' and calculate the average 'tx10pETCCDI' for each date\n",
    "    monthly_avg = dataframe.groupby('date')['tx10pETCCDI'].mean().reset_index()\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(monthly_avg['date'], monthly_avg['tx10pETCCDI'], marker='o', linestyle='-')\n",
    "    plt.title('Average tx10pETCCDI by Year-Month')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average tx10pETCCDI')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your DataFrame is named df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Specify the path to your shapefile or other spatial data file\n",
    "file_path = '/Users/gbenz/Downloads/pg_extent/pgm_viewser_extent.shp'\n",
    "\n",
    "# Load the data into a GeoDataFrame\n",
    "gdf = gpd.read_file(file_path)\n",
    "\n",
    "spatial_extent = len(pd.unique(gdf['gid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This prints a summary of Null values:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gid             0\n",
       "year            0\n",
       "month           0\n",
       "date            0\n",
       "cddETCCDI    2936\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of temporal and spatial units:\n",
      "\n",
      "This dataset expects to see 25 and found 4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spatial_extent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/gbenz/Documents/Climate Data/climate_extremes/oct_processing.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThis dataset expects to see \u001b[39m\u001b[39m{\u001b[39;00mtime_length\u001b[39m}\u001b[39;00m\u001b[39m and found \u001b[39m\u001b[39m{\u001b[39;00metccdi_time_length\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThis dataset expects to see \u001b[39m\u001b[39m{\u001b[39;00mspatial_extent\u001b[39m}\u001b[39;00m\u001b[39m and found \u001b[39m\u001b[39m{\u001b[39;00metccdi_spatial_length\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gbenz/Documents/Climate%20Data/climate_extremes/oct_processing.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAverage over time to expose any temporal gaps\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spatial_extent' is not defined"
     ]
    }
   ],
   "source": [
    "validate_etccdi = pd.read_csv('/Users/gbenz/Documents/Climate Data/climate_extremes/etccdi_out_files/cddETCCDI_1990_06__1993_06.csv')\n",
    "validate_etccdi['date'] = validate_etccdi['year'].astype(str) + '-' + validate_etccdi['month'].astype(str).str.zfill(2)\n",
    "validate_etccdi['year'] = validate_etccdi['year'].astype(str)\n",
    "\n",
    "etccdi_time_length = len(pd.unique(validate_etccdi['date']))\n",
    "etccdi_spatial_length = len(pd.unique(validate_etccdi['gid']))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Report null values:\n",
    "\n",
    "# Specify the columns to check for null values\n",
    "columns_to_check = ['gid', 'year', 'month', 'date', etccdi_index]\n",
    "print('This prints a summary of Null values:')\n",
    "# Count null values in the specified columns\n",
    "null_counts = validate_etccdi[columns_to_check].isnull().sum()\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print()\n",
    "display(null_counts)\n",
    "\n",
    "print()\n",
    "print('Summary of temporal and spatial units:')\n",
    "print()\n",
    "print(f'This dataset expects to see {time_length} and found {etccdi_time_length}')\n",
    "print(f'This dataset expects to see {spatial_extent} and found {etccdi_spatial_length}')\n",
    "print()\n",
    "print('Average over time to expose any temporal gaps')\n",
    "#plot_monthly_average_tx10pETCCDI(validate_etccdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   priogrid_gid  year                                           geometry  \\\n",
      "0         62356  1990  POLYGON ((37.499999999800195 -47.0000000003996...   \n",
      "1         79599  1990  POLYGON ((18.999999999900126 -35.0000000000998...   \n",
      "2         79600  1990  POLYGON ((19.499999999800195 -35.0000000000998...   \n",
      "3         79601  1990  POLYGON ((19.999999999700265 -35.0000000000998...   \n",
      "4         80317  1990  POLYGON ((18.000000000099988 -34.5000000001998...   \n",
      "\n",
      "     gid  xcoord  ycoord  cddETCCDI  month     date  \n",
      "0  62356   37.75  -46.75   8.174884      6  1990-06  \n",
      "1  79599   19.25  -34.75  22.465351      6  1990-06  \n",
      "2  79600   19.75  -34.75  21.393824      6  1990-06  \n",
      "3  79601   20.25  -34.75  21.253667      6  1990-06  \n",
      "4  80317   18.25  -34.25  28.487264      6  1990-06  \n"
     ]
    }
   ],
   "source": [
    "# Perform a left join on the 'priogrid_gid' and 'gid' columns, and 'year'\n",
    "merged_df = reference_filtered_time.merge(\n",
    "    validate_etccdi,\n",
    "    how='left',  # Keeps all rows from reference_filtered_time\n",
    "    left_on=['priogrid_gid', 'year'],  # Columns in reference_filtered_time\n",
    "    right_on=['gid', 'year'],  # Columns in validate_etccdi\n",
    "    suffixes=('_reference', '_validate')  # Optional: Adds suffixes to distinguish overlapping columns\n",
    ")\n",
    "\n",
    "# Check the result\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       priogrid_gid  year  cddETCCDI\n",
      "7472         154458  1990        NaN\n",
      "7473         154459  1990        NaN\n",
      "7474         154460  1990        NaN\n",
      "7475         154461  1990        NaN\n",
      "7476         154462  1990        NaN\n",
      "...             ...   ...        ...\n",
      "50604        173221  1993        NaN\n",
      "50605        173222  1993        NaN\n",
      "50606        173223  1993        NaN\n",
      "50607        173224  1993        NaN\n",
      "50608        173225  1993        NaN\n",
      "\n",
      "[2936 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the merged DataFrame to find rows where the 'cddETCCDI' field is NaN\n",
    "null_cddETCCDI_rows = merged_df[merged_df['cddETCCDI'].isna()]\n",
    "\n",
    "# Get a summary of the rows with NaN in the 'cddETCCDI' field\n",
    "summary_null_cddETCCDI = null_cddETCCDI_rows[['priogrid_gid', 'year', 'cddETCCDI']]  # Adjust columns to display\n",
    "print(summary_null_cddETCCDI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to Fine (Granular) Pixel \n",
    "\n",
    "Parameters to consider: \n",
    "1. What is an appropriate resolution\n",
    "2. What is the most appropriate resampling method\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
